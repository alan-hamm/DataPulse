{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Preprocessing for Unified Topic Modeling and Analysis (UTMA) Pipeline\n",
    "\n",
    "This notebook serves as the preprocessing stage for the Unified Topic Modeling and Analysis (UTMA) pipeline. The UTMA pipeline, primarily driven by `utma.py`, is designed to perform topic modeling, diachronic analysis, and visualization on diverse textual corpora. This notebook prepares documents for efficient intake by `utma.py`, ensuring compatibility with the UTMA pipeline's requirements and maintaining consistency across different data sources.\n",
    "\n",
    "### Notebook Objectives\n",
    "- **Load and Process Documents**: Reads in documents from various formats (e.g., HTML, JSON) and performs necessary transformations to standardize the content.\n",
    "- **Curly Quote Replacement and Validation**: Cleans up common issues such as curly quotes and non-printable characters to ensure text integrity.\n",
    "- **Extract and Organize Content**: Utilizes BeautifulSoup and regex to extract relevant content based on tag configurations, organizing text into paragraphs and sentences.\n",
    "- **Output for UTMA Intake**: Produces a structured, cleaned corpus that the `utma.py` script can ingest directly, enabling downstream analysis and visualization steps in the UTMA pipeline.\n",
    "\n",
    "### Dependencies and Related Scripts\n",
    "This notebook works in conjunction with several key components of the UTMA project:\n",
    "- **utma.py**: The main script for orchestrating topic modeling, diachronic analysis, and evaluation.\n",
    "- **alpha_eta.py**: Supports tuning of hyperparameters such as alpha and eta for topic modeling.\n",
    "- **process_futures.py**: Manages asynchronous processing, crucial for efficient handling of large corpora.\n",
    "- **topic_model_trainer.py**: Defines and trains the topic models.\n",
    "- **visualization.py**: Generates visualizations for model analysis.\n",
    "- **write_to_postgres.py**: Handles the persistence of processed data into PostgreSQL databases for structured storage and querying.\n",
    "- **utils.py**: Provides utility functions used across the UTMA pipeline for consistency and efficiency.\n",
    "\n",
    "### Workflow Overview\n",
    "1. **Document Loading**: Import HTML or JSON files, detecting encoding where necessary.\n",
    "2. **Content Extraction and Preprocessing**: Extracts paragraphs and sentences, normalizes punctuation, and replaces curly quotes to enhance text consistency.\n",
    "3. **Output for UTMA Intake**: Saves the preprocessed content in a structured format ready for direct intake by `utma.py`.\n",
    "\n",
    "By running this notebook, youâ€™ll prepare a clean, standardized corpus compatible with the UTMA pipeline, ensuring optimal input quality for topic modeling and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the user for permission to check and install required packages\n",
    "response = input(\"Do you want to check that the required packages, including the correct versions, are installed? (yes/no): \").lower()\n",
    "check = response in [\"yes\", \"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "import nltk\n",
    "# Required libraries with their respective install names and versions (if needed)\n",
    "libraries = {\n",
    "    \"nltk\": {\"install_name\": \"nltk\", \"version\": \"3.6.5\", \"nltk_data\": \"stopwords\"},\n",
    "    \"bs4\": {\"install_name\": \"beautifulsoup4\"},\n",
    "    \"gensim\": {\"install_name\": \"gensim\", \"version\": \"4.0.1\"},\n",
    "    \"readability.readability\": {\"install_name\": \"readability-lxml\"},\n",
    "    \"html5lib\": {\"install_name\": \"html5lib\"},\n",
    "    \"tqdm\": {\"install_name\": \"tqdm\", \"version\": \"4.62.3\"},\n",
    "    \"sklearn.manifold\": {\"install_name\": \"scikit-learn\"},\n",
    "    \"matplotlib\": {\"install_name\": \"matplotlib\", \"version\": \"3.4.3\"},\n",
    "    \"numpy\": {\"install_name\": \"numpy\", \"version\": \"1.21.2\"},\n",
    "    \"pandas\": {\"install_name\": \"pandas\", \"version\": \"1.3.3\"},\n",
    "    \"spacy\": {\"install_name\": \"spacy\", \"spacy_model\": \"en_core_web_lg\"},\n",
    "}\n",
    "\n",
    "def check_and_install(package_name, install_name=None, spacy_model=None, version=None, nltk_data=None):\n",
    "    \"\"\"\n",
    "    Checks if a package is installed; if not, prompts the user for permission to install it.\n",
    "    Supports specifying version, spaCy model downloads, and NLTK datasets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        importlib.import_module(package_name)\n",
    "        if spacy_model:  # Special case for spaCy models\n",
    "            import spacy\n",
    "            if not spacy.util.is_package(spacy_model):\n",
    "                raise ImportError(f\"{spacy_model} model not found\")\n",
    "        if nltk_data:  # Check for required NLTK data (e.g., stopwords)\n",
    "            nltk.data.find(f\"corpora/{nltk_data}\")\n",
    "    except (ImportError, LookupError):\n",
    "        response = input(f\"'{package_name}' or required data is missing. Would you like to install it? (yes/no): \").strip().lower()\n",
    "        if response in [\"yes\", \"y\"]:\n",
    "            if install_name:\n",
    "                install_command = [sys.executable, \"-m\", \"pip\", \"install\"]\n",
    "                if version:\n",
    "                    install_command.append(f\"{install_name}=={version}\")\n",
    "                else:\n",
    "                    install_command.append(install_name)\n",
    "                subprocess.check_call(install_command)\n",
    "            if spacy_model:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", spacy_model])\n",
    "            if nltk_data:\n",
    "                nltk.download(nltk_data)\n",
    "        else:\n",
    "            print(f\"Skipping installation of '{package_name}' or required data. The program may not run as expected.\")\n",
    "\n",
    "def check_libraries(check=True):\n",
    "    if not check:\n",
    "        print(\"Skipping package check and installation as per user input.\")\n",
    "        return\n",
    "    \n",
    "    for package_name, details in libraries.items():\n",
    "        check_and_install(\n",
    "            package_name,\n",
    "            install_name=details.get(\"install_name\"),\n",
    "            spacy_model=details.get(\"spacy_model\"),\n",
    "            version=details.get(\"version\"),\n",
    "            nltk_data=details.get(\"nltk_data\")\n",
    "        )\n",
    "\n",
    "# Call to check and install libraries if needed\n",
    "check_libraries(check=check)\n",
    "\n",
    "# Now you can use the stopwords\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DOC_ID = r'.*[\\d\\w\\-.]+\\.(html|json)$'\n",
    "\n",
    "\n",
    "\n",
    "TAGS = ['p']\n",
    "\n",
    "\n",
    "# stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# observed MMWR Journal findings\n",
    "\n",
    "stop_words.extend(['icon', 'website', 'mmwr', 'citation', 'author', 'report', 'formatting', \"format\",'regarding',\n",
    "                   'system', 'datum', 'link', 'linking', 'federal', 'data', 'tract', 'census', 'study',\"question\",\n",
    "                   'conduct', 'report', 'including', 'top', 'summary', 'however', 'name', 'known', 'figure', 'return', \n",
    "                   'page', 'view', 'affiliation', 'pdf', 'law', 'version', 'list', 'endorsement', \"review\",\n",
    "                   'article', 'download', 'reference', 'publication', 'discussion', 'table', 'vol', \"message\",\n",
    "                   'information', 'web', 'notification', 'policy', 'policie', #spaCy lemmatization can make errors with pluralization(e.g. rabie for rabies)\n",
    "                   'acknowledgment', 'altmetric', 'health',\n",
    "                   'abbreviation', 'figure', \"service\",\"imply\",\"current\",\"source\",\n",
    "                   \"trade\",\"address\", \"addresses\",\"program\",\"organization\" ,\"provided\", \"copyrighted\", \"copyright\",\n",
    "                   \"already\", \"topic\", \"art\", 'e.g', 'eg',\n",
    "                   'generated', 'proofs', 'automated', 'process', 'conversion', 'result', 'character', 'translation', 'errors', 'referred', 'electronic', 'original', 'printable', 'official',\n",
    "                   'Use', 'of', 'trade', 'names', 'and', 'commercial', 'sources', 'is', 'for', 'identification', 'only', 'and', 'does', 'not', 'imply', 'endorsement', 'by', 'the', 'Department', 'of', \n",
    "                   'Health', 'and', 'Human', 'Services', 'References', 'to', 'CDC', 'sites', 'on', 'the', 'Internet', 'are', 'provided', 'as', 'a', 'service', 'to', 'MMWR', 'readers', 'and', 'do', \n",
    "                   'not', 'constitute', 'or', 'imply', 'endorsement', 'of', 'these', 'organizations', 'or', 'their', 'programs', 'by', 'CDC', 'or', 'the', 'Department', 'of', 'Health', 'and', \n",
    "                   'Human', 'Services', 'CDC', 'is', 'not', 'responsible', 'for', 'the', 'content', 'of', 'pages', 'found', 'at', 'these', 'sites', 'URL', 'addresses', 'listed', 'in', 'MMWR', \n",
    "                   'were', 'current', 'as', 'of', 'the', 'date', 'of', 'publication'])\n",
    "# pretrained model for POS tagging/filtering\n",
    "nlp = en_core_web_lg.load( disable=['parser','ner'])\n",
    "\n",
    "TAGS = ['p']\n",
    "\n",
    "# set encoding for CorpusReader class\n",
    "ENCODING = 'latin1'\n",
    "\n",
    "# SET DIR PATHS\n",
    "JSON_OUT = r\"C:\\topic-modeling\\data\\documents\\2015\"\n",
    "\n",
    "# set value to determine if lemmatization will be performed\n",
    "LEMMATIZATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from time import time\n",
    "from hashlib import md5\n",
    "import chardet\n",
    "import unicodedata\n",
    "from nltk.corpus.reader.api import CorpusReader  # Only import CorpusReader now\n",
    "\n",
    "class HTMLReader(CorpusReader):\n",
    "    \n",
    "    def __init__(self, root, tags=TAGS, fileids=DOC_ID, **kwargs):\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "        self.tags = tags\n",
    "\n",
    "    def resolve(self, fileids=None):\n",
    "        return fileids  # Simplified as we no longer need categories\n",
    "\n",
    "    def detect_encoding(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "        return chardet.detect(raw_data)['encoding']\n",
    "    \n",
    "    def docs(self, fileids=None, pattern=None):\n",
    "        fileids = self.resolve(fileids)\n",
    "        \n",
    "        # Compile the regular expression pattern, or use a default if none is provided\n",
    "        if pattern is not None:\n",
    "            regex = re.compile(pattern, re.IGNORECASE)\n",
    "        else:\n",
    "            regex = re.compile(r'.*([\\d\\w\\-.]+)\\.(html|json)$', re.IGNORECASE)\n",
    "            \n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            if regex.search(path):\n",
    "                encoding = self.detect_encoding(path)\n",
    "                \n",
    "                # Check if the file is JSON by extension\n",
    "                if path.lower().endswith('.json'):\n",
    "                    with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                        try:\n",
    "                            # Load JSON content\n",
    "                            data = json.load(f)\n",
    "                            # If the JSON contains a list of HTML strings or lists of strings, process each one\n",
    "                            if isinstance(data, list):\n",
    "                                for item in data:\n",
    "                                    if isinstance(item, str):  # Single HTML string\n",
    "                                        #print(f\"Yielding HTML from JSON: {item[:30]}...\")  # Debug print\n",
    "                                        yield self.replace_curly_quotes(item)\n",
    "                                    elif isinstance(item, list):  # Nested list of strings\n",
    "                                        for html_content in item:\n",
    "                                            if isinstance(html_content, str):\n",
    "                                                #print(f\"Yielding nested HTML from JSON: {html_content[:30]}...\")  # Debug print\n",
    "                                                yield self.replace_curly_quotes(html_content)\n",
    "                            else:\n",
    "                                print(f\"Error: {path} does not contain a list of HTML strings.\")\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(f\"Error: {path} is not a valid JSON file.\")\n",
    "                \n",
    "                elif path.lower().endswith('.html'):\n",
    "                    # Process as regular HTML file\n",
    "                    with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                        doc_content = f.read()\n",
    "                        #print(f\"Yielding HTML from file: {doc_content[:30]}...\")  # Debug print\n",
    "                        yield self.replace_curly_quotes(doc_content)\n",
    "                else:\n",
    "                    print(f\"Unsupported file type: {path}. Only JSON and HTML files are allowed.\")\n",
    "                    continue\n",
    "\n",
    "    def html(self, fileids=None):\n",
    "        for doc in self.docs(fileids):\n",
    "            try:\n",
    "                yield doc\n",
    "            except Exception as e:\n",
    "                print(\"Could not parse HTML: {}\".format(e))\n",
    "                continue\n",
    "\n",
    "    def replace_curly_quotes(self, text):\n",
    "        quote_replacements = {\n",
    "            u\"\\\\u2018\": \"'\",  # Left single quotation mark\n",
    "            u\"\\\\u2019\": \"'\",  # Right single quotation mark\n",
    "            u\"\\\\u201C\": '\"',  # Left double quotation mark\n",
    "            u\"\\\\u201D\": '\"',  # Right double quotation mark\n",
    "        }\n",
    "        \n",
    "        for curly_quote, straight_quote in quote_replacements.items():\n",
    "            text = text.replace(curly_quote, straight_quote)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def remove_non_printable_chars(self, text):\n",
    "        non_printable_pattern = re.compile(r'[\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]+')\n",
    "        cleaned_text = re.sub(non_printable_pattern, '', text)\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "    def get_invalid_character_names(self, text):\n",
    "        char_names = set()\n",
    "        non_printable_pattern = re.compile(r'[\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]')\n",
    "        invalid_chars = non_printable_pattern.findall(text)\n",
    "        for char in invalid_chars:\n",
    "            try:\n",
    "                name = unicodedata.name(char)\n",
    "            except ValueError:\n",
    "                name = \"UNKNOWN CONTROL CHARACTER\"\n",
    "            char_names.add(name)\n",
    "        return char_names\n",
    "        \n",
    "    def validate_paragraph(self, paragraph):\n",
    "        reasons = []\n",
    "        if not paragraph.strip():\n",
    "            reasons.append(\"Only whitespace\")\n",
    "\n",
    "        invalid_char_names = self.get_invalid_character_names(paragraph)\n",
    "        \n",
    "        if invalid_char_names:\n",
    "            reason = f\"Contains non-printable characters: {', '.join(invalid_char_names)}\"\n",
    "            reasons.append(reason)\n",
    "\n",
    "        return True if not reasons else ', '.join(reasons)\n",
    "    \n",
    "    para_dict = dict()\n",
    "    def paras(self, parser_type='lxml', fileids=None):\n",
    "        for html in self.html(fileids):\n",
    "            # Check if html content looks like an HTML string\n",
    "            if not isinstance(html, str) or \"<\" not in html:\n",
    "                print(f\"Skipping non-HTML content: {html}\")\n",
    "                continue\n",
    "            \n",
    "            soup = BeautifulSoup(html, parser_type)\n",
    "            \n",
    "            # Join tags into a CSS selector if `self.tags` is a list\n",
    "            tag_selector = \",\".join(self.tags) if isinstance(self.tags, list) else self.tags\n",
    "            \n",
    "            for element in soup.select(tag_selector):\n",
    "                text = element.text.strip()\n",
    "                yield text\n",
    "\n",
    "\n",
    "\n",
    "    sent_dict = dict()\n",
    "    def sents(self, fileids=None):\n",
    "        for paragraph in self.paras(fileids=fileids):\n",
    "            for sentence in nltk.sent_tokenize(paragraph): \n",
    "                yield sentence\n",
    "\n",
    "    word_dict = dict()\n",
    "    def words(self, fileids=None): \n",
    "        for sentence in self.sents(fileids=fileids):\n",
    "            for token in nltk.wordpunct_tokenize(sentence):\n",
    "                yield token\n",
    "\n",
    "    def generate(self, fileids=None, log_file_path=None):\n",
    "        doc_dict = []\n",
    "        error_dict = []\n",
    "        count = 0\n",
    "        all_paragraph_count = 0 \n",
    "\n",
    "        with open(log_file_path, 'a') as log_file:\n",
    "            for idx, html_content in enumerate(self.paras(fileids=fileids)):\n",
    "                html_content = self.replace_curly_quotes(html_content)\n",
    "                validation_result = self.validate_paragraph(html_content)\n",
    "\n",
    "                if isinstance(validation_result, bool) and validation_result:  \n",
    "                    all_paragraph_count += 1\n",
    "                    doc_dict.append(html_content)\n",
    "                else:\n",
    "                    if not isinstance(validation_result, bool):  # If we have a string with failure reasons...\n",
    "                        log_file.write(f\"Invalid Paragraph {count}: {validation_result}\\\\n\")\n",
    "\n",
    "                        cleaned_html_content = self.remove_non_printable_chars(html_content)\n",
    "\n",
    "                        if isinstance(self.validate_paragraph(cleaned_html_content), bool):\n",
    "                            count += 1\n",
    "                            doc_dict.append(cleaned_html_content)\n",
    "                        else:\n",
    "                            error_dict.append(cleaned_html_content)\n",
    "\n",
    "        return doc_dict, error_dict, count, all_paragraph_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_corpus = HTMLReader(r'C:\\topic-modeling\\data\\documents\\2015')\n",
    "#print(_corpus.categories())\n",
    "_corpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tuple, errors, count, all_paragraph_count= _corpus.generate(log_file_path=r\"C:\\topic-modeling\\data\\documents\\2015\\paragraph_error.log\")\n",
    "print(count)\n",
    "print(all_paragraph_count)\n",
    "print(len(corpus_tuple))\n",
    "# 2010: 977438, 2011:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt in errors:\n",
    "    print((txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import spacy\n",
    "\n",
    "texts_out = []\n",
    "inner_text = []\n",
    "\n",
    "# number of stopwords found\n",
    "stopword_count = nltk.FreqDist()\n",
    "\n",
    "pp.pprint(f\"Executing POS/LEMMATIZATION({LEMMATIZATION})\")\n",
    "\n",
    "t = time()\n",
    "for paras in tqdm(corpus_tuple): #, total=9168220):\n",
    "    doc = nlp(paras)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']:\n",
    "            if len(token.text) > 5:\n",
    "                if token.text.lower() not in stop_words and token.lemma_.lower() not in stop_words: \n",
    "                    if LEMMATIZATION == False:\n",
    "                        inner_text.append(token.text) \n",
    "                    else:\n",
    "                        inner_text.append(token.lemma_) \n",
    "                else:\n",
    "                    if LEMMATIZATION == False:\n",
    "                        stopword_count[token.text] += 1\n",
    "                    else:\n",
    "                        stopword_count[token.lemma_] += 1\n",
    "\n",
    "    if len(inner_text) > 0:\n",
    "        texts_out.append(inner_text)\n",
    "    inner_text = []\n",
    "\n",
    "#pp.pprint(texts_out)\n",
    "pp.pprint('Time to finish spaCy filter: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_for_processing =[]\n",
    "for sent in texts_out:\n",
    "    if len(sent) > 5:\n",
    "        texts_for_processing.append(sent)\n",
    "del texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_tokenized_sentences_to_jsonl(sentences, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            json_line = json.dumps(sentence, ensure_ascii=False)\n",
    "            f.write(json_line + '\\n')\n",
    "\n",
    "write_tokenized_sentences_to_jsonl(texts_for_processing, r'C:\\topic-modeling\\data\\documents\\2015\\2015.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fliename2 = r\"C:\\topic-modeling\\data\\documents\\2015\\2015.json')\"\n",
    "with open(fliename2, 'w') as jsonfile:\n",
    "    json.dump(texts_for_processing, jsonfile, indent=1, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(texts_for_processing)\n",
    "\n",
    "# freqDist object for bigrams\n",
    "bigram_freq = nltk.FreqDist()\n",
    "\n",
    "# print bigrams\n",
    "for ngrams, _ in bigram.vocab.items():\n",
    "    #unicode_ngrams = ngrams.decode('utf-8')\n",
    "    if '_' in ngrams:\n",
    "        bigram_freq[ngrams]+=1\n",
    "        print(ngrams)\n",
    "\n",
    "# add bigrams to texts_out to be included in corpus\n",
    "for idx in range(len(texts_for_processing)):\n",
    "    for token in bigram[texts_for_processing[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            texts_for_processing[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp.pprint(texts_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fliename3 = r\"C:\\topic-modeling\\data\\documents\\2015\\2015-w-bigrams.json\"\n",
    "with open(fliename3, 'w') as jsonfile:\n",
    "    json.dump(texts_for_processing, jsonfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tokenized_sentences_to_jsonl(texts_for_processing, r\"C:\\topic-modeling\\data\\documents\\2015\\2015-w-bigrams.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
