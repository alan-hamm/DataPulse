{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Preprocessing Notebook for Unified Topic Modeling and Analysis (UTMA) Pipeline\n",
    "\n",
    "This notebook performs document preprocessing for the Unified Topic Modeling and Analysis (UTMA) pipeline, ensuring documents are prepared for seamless ingestion by `utma.py` and related scripts. The UTMA pipeline integrates topic modeling, diachronic analysis, and visualization, allowing for adaptable and detailed analysis across diverse textual corpora. This notebook ensures document standardization, facilitating compatibility with UTMA's topic modeling and evaluation stages.\n",
    "\n",
    "### Notebook Objectives\n",
    "- **Load and Transform Documents**: Imports and structures text from HTML and JSON files, preparing them for UTMA's topic analysis.\n",
    "- **Data Cleansing**: Standardizes text by removing curly quotes, non-printable characters, and other inconsistencies.\n",
    "- **Content Structuring**: Extracts text using BeautifulSoup and regex, arranging it into paragraphs and sentences.\n",
    "- **Output Preparation**: Produces a processed corpus in a format optimized for `utma.py` ingestion, supporting downstream analysis and visualization.\n",
    "\n",
    "### Dependencies and Related Components\n",
    "This notebook is designed to work alongside the following scripts:\n",
    "- **`utma.py`**: Coordinates topic modeling, diachronic analysis, and evaluation.\n",
    "- **`alpha_eta.py`**: Supports hyperparameter tuning for model optimization.\n",
    "- **`process_futures.py`**: Manages asynchronous processing, essential for handling large datasets efficiently.\n",
    "- **`topic_model_trainer.py`**: Defines and trains the topic models used in UTMA.\n",
    "- **`visualization.py`**: Generates visualizations for model insights and evaluation.\n",
    "- **`write_to_postgres.py`**: Facilitates data persistence into PostgreSQL, supporting structured data retrieval.\n",
    "- **`utils.py`**: Provides utility functions to enhance efficiency and consistency across the UTMA pipeline.\n",
    "\n",
    "### Workflow Overview\n",
    "1. **Document Loading**: Reads HTML or JSON files, detecting encoding where necessary.\n",
    "2. **Content Extraction**: Extracts structured content, normalizing punctuation and replacing curly quotes for text consistency.\n",
    "3. **Data Output**: Saves the processed content in a structured format for direct use in `utma.py`.\n",
    "\n",
    "Running this notebook prepares a clean, standardized corpus compatible with the UTMA pipeline, optimizing input quality for topic modeling and diachronic analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup Instructions\n",
    "\n",
    "> ⚠️ **Important:** Before running the package installation steps in this notebook, you must first create the Conda environment from the Anaconda Prompt.\n",
    ">\n",
    "> 1. **Open Anaconda Prompt**.\n",
    "> 2. **Create the Environment**: Run the following command in Anaconda Prompt:\n",
    ">    ```bash\n",
    ">    conda create --name UTMA python=3.12.0\n",
    ">    ```\n",
    "> 3. **Verify Environment Creation**: Check that the `UTMA` environment was created by listing all environments:\n",
    ">    ```bash\n",
    ">    conda env list\n",
    ">    ```\n",
    ">\n",
    "> 4. **Exit the Notebook** and restart it in the newly created `UTMA` environment. Then, return to this notebook to proceed with the installation steps and install packages in `UTMA` using the provided requirements file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that you're using the newly created environment\n",
    "%pip install -r requirements.txt\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os            # Provides functions for interacting with the operating system, e.g., file handling.\n",
    "import re            # Supports regular expressions for text manipulation and pattern matching.\n",
    "import csv           # Facilitates reading from and writing to CSV files.\n",
    "import json          # Enables reading from and writing to JSON files, often used for structured data.\n",
    "from time import time  # Allows timing operations for performance measurement.\n",
    "import logging       # Provides logging functionalities to monitor code execution.\n",
    "import codecs        # Handles different text encodings, important for text data processing.\n",
    "import multiprocessing  # Supports parallel processing, enhancing performance on large datasets.\n",
    "import pprint as pp  # Pretty-prints data structures, useful for debugging and visualization.\n",
    "from hashlib import md5  # Provides MD5 hashing, useful for generating unique IDs or checking data integrity.\n",
    "\n",
    "# Encoding and Parsing Imports\n",
    "import chardet        # Detects character encoding of text files, allowing for accurate reading of various encodings.\n",
    "import unicodedata    # Handles Unicode character information, useful for identifying and removing non-printable characters.\n",
    "from bs4 import BeautifulSoup  # Parses HTML content, enabling extraction of specific tags (e.g., <p> tags) for processing.\n",
    "\n",
    "\n",
    "# NLTK Imports\n",
    "import nltk                             # Natural Language Toolkit, a suite for text processing.\n",
    "from nltk.corpus import stopwords       # Provides lists of stop words to remove from text.\n",
    "from nltk.corpus.reader.api import CorpusReader  # Base class for reading and structuring corpora.\n",
    "from nltk import sent_tokenize, pos_tag, wordpunct_tokenize  # Tokenizers and POS tagger for sentence processing.\n",
    "stop_words = stopwords.words('english')  # Initializing English stop words list for filtering out common words.\n",
    "\n",
    "# Gensim Imports\n",
    "import gensim                           # Library for topic modeling and word vector creation.\n",
    "from gensim.models import Word2Vec, ldamulticore  # Word2Vec for word embeddings, ldamulticore for topic modeling.\n",
    "from gensim.models.phrases import Phrases, Phraser  # Constructs multi-word phrases (e.g., bigrams) from tokenized text.\n",
    "import gensim.corpora as corpora        # Handles creation of dictionaries and corpora from text data.\n",
    "from gensim.utils import simple_preprocess  # Preprocesses text into a list of tokens.\n",
    "\n",
    "# SpaCy Import (specific model)\n",
    "import en_core_web_lg                   # SpaCy's large English NLP model for advanced text processing.\n",
    "nlp = en_core_web_lg.load(disable=['parser','ner'])  # Loads model, with parsing and named entity recognition disabled.\n",
    "\n",
    "# Readability Import\n",
    "from readability.readability import Unparseable  # Exception handling for parsing errors in HTML.\n",
    "from readability.readability import Document as Paper  # Extracts readable content from HTML, discarding noise.\n",
    "\n",
    "# BeautifulSoup and HTML5lib\n",
    "import bs4                                # BeautifulSoup for parsing HTML and XML documents.\n",
    "import html5lib                           # Parser for HTML5, used by BeautifulSoup for web scraping.\n",
    "\n",
    "# Data Processing and Scientific Libraries\n",
    "import numpy as np                        # Supports efficient numerical operations on large arrays and matrices.\n",
    "import pandas as pd                       # Data analysis library for handling structured data (e.g., DataFrames).\n",
    "from sklearn.manifold import TSNE         # Dimensionality reduction for visualizing high-dimensional data.\n",
    "from matplotlib import pyplot as plt      # Plotting library for creating visualizations.\n",
    "from tqdm import tqdm                     # Adds progress bars to loops, useful for monitoring lengthy operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom stop words based on document context\n",
    "with open('config/custom_stopwords.json', 'r') as file:\n",
    "    custom_stopwords = json.load(file)\n",
    "\n",
    "# Add CDC MMWR-specific stop words, or any additional terms the user finds appropriate\n",
    "# This approach enables the user to include context-specific stop words, not limited to MMWR.\n",
    "# Users can add any terms they consider irrelevant or noise within the custom_stopwords.json file, \n",
    "# making this process adaptable to various document types or specific text analysis needs.\n",
    "stop_words.update(custom_stopwords.get(\"cdc_mmwr\", []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_ID = r'.*[\\d\\w\\-.]+\\.(html|json)$'  # Regular expression pattern to identify document filenames ending in .html or .json.\n",
    "\n",
    "TAGS = ['p']  # List of HTML tags to extract content from; 'p' is commonly used to denote paragraphs in HTML.\n",
    "\n",
    "# Set flag to determine if lemmatization (reducing words to their base form) will be performed during preprocessing.\n",
    "LEMMATIZATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from time import time\n",
    "from hashlib import md5\n",
    "import chardet\n",
    "import unicodedata\n",
    "from nltk.corpus.reader.api import CorpusReader  # Only import CorpusReader now\n",
    "\n",
    "class HTMLReader(CorpusReader):\n",
    "    \n",
    "    def __init__(self, root, tags=TAGS, fileids=DOC_ID, **kwargs):\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "        self.tags = tags\n",
    "\n",
    "    def resolve(self, fileids=None):\n",
    "        return fileids  # Simplified as we no longer need categories\n",
    "\n",
    "    def detect_encoding(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "        return chardet.detect(raw_data)['encoding']\n",
    "    \n",
    "    def process_content(self, content, non_html_log_file):\n",
    "        \"\"\"\n",
    "        Process content based on whether it contains HTML tags.\n",
    "        If HTML tags are detected, extract <p> tags; otherwise, treat it as plain text.\n",
    "        \"\"\"\n",
    "        content = content.replace(\"\\\\n\", \"\\n\").strip()  # Clean escape sequences\n",
    "\n",
    "        # Check for specific HTML tags (like <p>) to determine if it's HTML\n",
    "        if \"<p>\" in content or \"</p>\" in content:\n",
    "            # Process as HTML\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            for p in paragraphs:\n",
    "                yield p.get_text()\n",
    "        else:\n",
    "            # Log non-HTML content to the specified log file\n",
    "            non_html_log_file.write(f\"Skipping non-HTML content: {content[:100]}...\\n\")  # Write first 100 chars as a preview\n",
    "            yield None  # Skip processing this as valid content\n",
    "\n",
    "    def docs(self, fileids=None, pattern=None, non_html_log_file=None):\n",
    "        fileids = self.resolve(fileids)\n",
    "        \n",
    "        # Compile the regular expression pattern, or use a default if none is provided\n",
    "        if pattern is not None:\n",
    "            regex = re.compile(pattern, re.IGNORECASE)\n",
    "        else:\n",
    "            regex = re.compile(r'.*([\\d\\w\\-.]+)\\.(html|json)$', re.IGNORECASE)\n",
    "            \n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            if regex.search(path):\n",
    "                encoding = self.detect_encoding(path)\n",
    "                \n",
    "                # Check if the file is JSON by extension\n",
    "                if path.lower().endswith('.json'):\n",
    "                    with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                        try:\n",
    "                            # Load JSON content\n",
    "                            data = json.load(f)\n",
    "                            # Process each HTML snippet in JSON directly for <p> tags\n",
    "                            if isinstance(data, list):\n",
    "                                for item in data:\n",
    "                                    if isinstance(item, str):\n",
    "                                        # Process each content item with HTML or plain text\n",
    "                                        for content in self.process_content(item, non_html_log_file):\n",
    "                                            if content:\n",
    "                                                yield self.replace_curly_quotes(content)\n",
    "                            else:\n",
    "                                print(f\"Error: {path} does not contain a list of HTML strings.\")\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(f\"Error: {path} is not a valid JSON file.\")\n",
    "                \n",
    "                elif path.lower().endswith('.html'):\n",
    "                    # Process as regular HTML file\n",
    "                    with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                        doc_content = f.read()\n",
    "                        # Process as HTML or plain text\n",
    "                        for content in self.process_content(doc_content, non_html_log_file):\n",
    "                            if content:\n",
    "                                yield self.replace_curly_quotes(content)\n",
    "                else:\n",
    "                    print(f\"Unsupported file type: {path}. Only JSON and HTML files are allowed.\")\n",
    "                    continue\n",
    "\n",
    "    def html(self, fileids=None):\n",
    "        for doc in self.docs(fileids):\n",
    "            try:\n",
    "                yield doc\n",
    "            except Exception as e:\n",
    "                print(\"Could not parse HTML: {}\".format(e))\n",
    "                continue\n",
    "\n",
    "    def replace_curly_quotes(self, text):\n",
    "        quote_replacements = {\n",
    "            u\"\\\\u2018\": \"'\",  # Left single quotation mark\n",
    "            u\"\\\\u2019\": \"'\",  # Right single quotation mark\n",
    "            u\"\\\\u201C\": '\"',  # Left double quotation mark\n",
    "            u\"\\\\u201D\": '\"',  # Right double quotation mark\n",
    "        }\n",
    "        \n",
    "        for curly_quote, straight_quote in quote_replacements.items():\n",
    "            text = text.replace(curly_quote, straight_quote)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def remove_non_printable_chars(self, text):\n",
    "        non_printable_pattern = re.compile(r'[\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]+')\n",
    "        cleaned_text = re.sub(non_printable_pattern, '', text)\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "    def get_invalid_character_names(self, text):\n",
    "        char_names = set()\n",
    "        non_printable_pattern = re.compile(r'[\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]')\n",
    "        invalid_chars = non_printable_pattern.findall(text)\n",
    "        for char in invalid_chars:\n",
    "            try:\n",
    "                name = unicodedata.name(char)\n",
    "            except ValueError:\n",
    "                name = \"UNKNOWN CONTROL CHARACTER\"\n",
    "            char_names.add(name)\n",
    "        return char_names\n",
    "        \n",
    "    def validate_paragraph(self, paragraph):\n",
    "        reasons = []\n",
    "        if not paragraph.strip():\n",
    "            reasons.append(\"Only whitespace\")\n",
    "\n",
    "        invalid_char_names = self.get_invalid_character_names(paragraph)\n",
    "        \n",
    "        if invalid_char_names:\n",
    "            reason = f\"Contains non-printable characters: {', '.join(invalid_char_names)}\"\n",
    "            reasons.append(reason)\n",
    "\n",
    "        return True if not reasons else ', '.join(reasons)\n",
    "    \n",
    "    para_dict = dict()\n",
    "    def paras(self, parser_type='lxml', fileids=None):\n",
    "        for html in self.html(fileids):\n",
    "            # Check if html content looks like an HTML string\n",
    "            if not isinstance(html, str) or \"<\" not in html:\n",
    "                print(f\"Skipping non-HTML content: {html}\")\n",
    "                continue\n",
    "            \n",
    "            soup = BeautifulSoup(html, parser_type)\n",
    "            \n",
    "            # Join tags into a CSS selector if `self.tags` is a list\n",
    "            tag_selector = \",\".join(self.tags) if isinstance(self.tags, list) else self.tags\n",
    "            \n",
    "            for element in soup.select(tag_selector):\n",
    "                text = element.text.strip()\n",
    "                yield text\n",
    "\n",
    "    sent_dict = dict()\n",
    "    def sents(self, fileids=None):\n",
    "        for paragraph in self.paras(fileids=fileids):\n",
    "            for sentence in nltk.sent_tokenize(paragraph): \n",
    "                yield sentence\n",
    "\n",
    "    word_dict = dict()\n",
    "    def words(self, fileids=None): \n",
    "        for sentence in self.sents(fileids=fileids):\n",
    "            for token in nltk.wordpunct_tokenize(sentence):\n",
    "                yield token\n",
    "\n",
    "    def generate(self, fileids=None, log_file_path=None, non_html_log_path=None):\n",
    "        doc_dict = []\n",
    "        error_dict = []\n",
    "        count = 0\n",
    "        all_paragraph_count = 0 \n",
    "\n",
    "        # Open two log files: one for invalid paragraphs and one for non-HTML content\n",
    "        with open(log_file_path, 'a') as invalid_log_file, open(non_html_log_path, 'a') as non_html_log_file:\n",
    "            for idx, html_content in enumerate(self.docs(fileids=fileids, non_html_log_file=non_html_log_file)):\n",
    "                html_content = self.replace_curly_quotes(html_content)\n",
    "                validation_result = self.validate_paragraph(html_content)\n",
    "\n",
    "                if isinstance(validation_result, bool) and validation_result:  \n",
    "                    # Valid paragraph\n",
    "                    all_paragraph_count += 1\n",
    "                    doc_dict.append(html_content)\n",
    "                else:\n",
    "                    # Invalid paragraph; log to the invalid paragraphs file\n",
    "                    if not isinstance(validation_result, bool):\n",
    "                        invalid_log_file.write(f\"Invalid Paragraph {count}: {validation_result}\\n\")\n",
    "                        cleaned_html_content = self.remove_non_printable_chars(html_content)\n",
    "\n",
    "                        if isinstance(self.validate_paragraph(cleaned_html_content), bool):\n",
    "                            count += 1\n",
    "                            doc_dict.append(cleaned_html_content)\n",
    "                        else:\n",
    "                            error_dict.append(cleaned_html_content)\n",
    "\n",
    "        return doc_dict, error_dict, count, all_paragraph_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Important:** Verify File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_corpus = HTMLReader(r'C:\\topic-modeling\\data\\docs-to-process\\2015')\n",
    "_corpus.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Important:** Verify file output location and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tuple, errors, count, all_paragraph_count = _corpus.generate(\n",
    "    log_file_path=r\"C:\\topic-modeling\\data\\docs-to-process\\2015\\paragraph_error.log\",\n",
    "    non_html_log_path=r\"C:\\topic-modeling\\data\\docs-to-process\\2015\\non_html_content.log\"\n",
    ")\n",
    "\n",
    "print(count)\n",
    "print(all_paragraph_count)\n",
    "print(len(corpus_tuple))\n",
    "# 2010: 977438, 2011:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt in errors:\n",
    "    print((txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import spacy\n",
    "\n",
    "texts_out = []\n",
    "inner_text = []\n",
    "\n",
    "# number of stopwords found\n",
    "stopword_count = nltk.FreqDist()\n",
    "\n",
    "pp.pprint(f\"Executing POS/LEMMATIZATION({LEMMATIZATION})\")\n",
    "\n",
    "t = time()\n",
    "for paras in tqdm(corpus_tuple): #, total=9168220):\n",
    "    doc = nlp(paras)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']:\n",
    "            if len(token.text) >= 5:\n",
    "                if token.text.lower() not in stop_words and token.lemma_.lower() not in stop_words: \n",
    "                    if LEMMATIZATION == False:\n",
    "                        inner_text.append(token.text) \n",
    "                    else:\n",
    "                        inner_text.append(token.lemma_) \n",
    "                else:\n",
    "                    if LEMMATIZATION == False:\n",
    "                        stopword_count[token.text] += 1\n",
    "                    else:\n",
    "                        stopword_count[token.lemma_] += 1\n",
    "\n",
    "    if len(inner_text) > 0:\n",
    "        texts_out.append(inner_text)\n",
    "    inner_text = []\n",
    "\n",
    "#pp.pprint(texts_out)\n",
    "pp.pprint('Time to finish spaCy filter: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_for_processing =[]\n",
    "for sent in texts_out:\n",
    "    if len(sent) > 5:\n",
    "        texts_for_processing.append(sent)\n",
    "del texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(texts_for_processing))\n",
    "for text in texts_for_processing:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Important:** Verify file output location and filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_tokenized_sentences_to_jsonl(sentences, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            json_line = json.dumps(sentence, ensure_ascii=False)\n",
    "            f.write(json_line + '\\n')\n",
    "\n",
    "write_tokenized_sentences_to_jsonl(texts_for_processing, r'C:\\topic-modeling\\data\\processed-docs\\2015\\2015.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Important:** Verify file output location and filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fliename2 = r\"C:\\topic-modeling\\data\\processed-docs\\2015\\2015.json')\"\n",
    "with open(fliename2, 'w') as jsonfile:\n",
    "    json.dump(texts_for_processing, jsonfile, indent=1, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(texts_for_processing)\n",
    "\n",
    "# freqDist object for bigrams\n",
    "bigram_freq = nltk.FreqDist()\n",
    "\n",
    "# print bigrams\n",
    "for ngrams, _ in bigram.vocab.items():\n",
    "    #unicode_ngrams = ngrams.decode('utf-8')\n",
    "    if '_' in ngrams:\n",
    "        bigram_freq[ngrams]+=1\n",
    "        print(ngrams)\n",
    "\n",
    "# add bigrams to texts_out to be included in corpus\n",
    "for idx in range(len(texts_for_processing)):\n",
    "    for token in bigram[texts_for_processing[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            texts_for_processing[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Important:** Verify file output location and filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fliename3 = r\"C:\\topic-modeling\\data\\processed-docs\\2015\\2015-w-bigrams.json\"\n",
    "with open(fliename3, 'w') as jsonfile:\n",
    "    json.dump(texts_for_processing, jsonfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Important:** Verify file output location and filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tokenized_sentences_to_jsonl(texts_for_processing, r\"C:\\topic-modeling\\data\\processed-docs\\2015\\2015-w-bigrams.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UTMA_TEST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
