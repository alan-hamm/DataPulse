{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ETL for Unified Topic Modeling Analysis (UTMA) - CDC MMWR Journals and Beyond**\n",
    "\n",
    "### **Author**: [Your Name]  \n",
    "### **Date**: [Date of Notebook Creation]\n",
    "\n",
    "---\n",
    "\n",
    "## **Notebook Overview**\n",
    "\n",
    "This notebook is part of the **Unified Topic Modeling Analysis (UTMA)** project, focusing on the **Extract, Transform, Load (ETL)** process for preparing textual data from various sources—including medical journals, novels, and general public health documents—for topic modeling and diachronic analysis.\n",
    "\n",
    "### **1. Objectives of the ETL Process**\n",
    "The primary purpose of this ETL notebook is to preprocess textual data from multiple time periods and sources in a manner that ensures high variability in the resulting corpora, which is crucial for effective topic modeling and subsequent diachronic analysis. Specifically, this notebook will:\n",
    "- **Combine Data from Multiple Time Periods**: Expand the temporal scope by merging different corpora (e.g., CDC MMWR publications from 2010-2014, 2014-2019, and 2020-2024) into a unified text to increase diversity.\n",
    "- **Balanced Sampling and Temporal Tagging**: Ensure that each document is tagged with its respective time period, enabling balanced representation across temporal spans during training.\n",
    "- **Diversity-Oriented Batch Preparation**: Implement strategies to diversify the data, such as weighted sampling and clustering, to mitigate the limited variance that impacts model quality.\n",
    "\n",
    "### **2. Data Sources**\n",
    "- **CDC MMWR Journals** (2010-2024): Split into three segments (2010-2014, 2014-2019, 2020-2024) to allow for **temporal analysis** and facilitate the understanding of changes in public health discourse.\n",
    "- **Additional Texts (Future Expansion)**: General-purpose pipeline to support documents such as novels, newspaper articles, and other public health records.\n",
    "\n",
    "### **3. Key Preprocessing Steps**\n",
    "This notebook will perform the following steps to ensure that the final output is prepared for topic modeling in a robust and variance-rich manner:\n",
    "- **Text Extraction**:\n",
    "  - Extract raw paragraphs from input documents and corpora, ensuring all parts of the data are captured.\n",
    "  \n",
    "- **Content Transformation**:\n",
    "  - **Tokenization**: Utilize spaCy’s NLP pipeline to tokenize the paragraphs.\n",
    "  - **Stop Word Handling**: Evaluate and experiment with retaining or removing stop words, considering their role in providing context and facilitating diachronic analysis.\n",
    "  - **POS Filtering and Character Length Adjustment**: Initially process only content-rich words (nouns, adjectives, verbs, adverbs) with 5+ characters, but allow flexibility to include other parts of speech or adjust the threshold based on analysis needs.\n",
    "  - **Temporal Tagging**: Tag paragraphs with their respective time period to retain temporal information for balanced sampling and diachronic analysis.\n",
    "\n",
    "### **4. Strategies for Variability Enhancement**\n",
    "Given the challenges of **limited variance** within homogenous corpora (e.g., CDC MMWR journals), this notebook employs multiple strategies to ensure variability:\n",
    "- **Balanced Sampling Across Time Periods**: Incorporate a balanced mix of paragraphs from all three time segments in each training batch.\n",
    "- **Diversity-Based Batch Formation and Weighted Sampling**: Use techniques like **TF-IDF-based clustering** and **weighted sampling** to diversify the batches, ensuring maximum differentiation between topics.\n",
    "  \n",
    "### **5. Data Outputs**\n",
    "- **Unified Corpus**: A unified text, comprising paragraphs from all three temporal segments, tagged accordingly.\n",
    "- **Segmented Corpora**: Individual segments for each time period, maintaining temporal integrity for diachronic analysis.\n",
    "- **Batch Preparation for Training**: Diverse and balanced batches of paragraphs ready for input into the **Gensim LDA model**, ensuring that variability is maximized to improve coherence, perplexity, and topic differentiation.\n",
    "\n",
    "### **6. Challenges Addressed in ETL**\n",
    "- **Limited Corpus Variance**: By expanding the data scope and employing balanced sampling and batch diversification, the goal is to address and mitigate the uniformity issues that limit the richness of LDA topics.\n",
    "- **Preparation for Future Analysis**: This notebook not only sets the foundation for robust LDA modeling but also prepares the data for **dynamic topic modeling (DTM)** and **hierarchical LDA (hLDA)**, to be implemented in subsequent stages for a richer understanding of topic evolution over time.\n",
    "\n",
    "### **7. Future Considerations**\n",
    "- **Data Augmentation**: Consider data augmentation techniques such as synonym replacement or back-translation to further enhance variability in future iterations.\n",
    "- **Advanced Modeling Approaches**: This ETL process will feed into subsequent modeling efforts, including experimenting with **Non-Negative Matrix Factorization (NMF)** and **hierarchical topic modeling**, as well as exploring **adaptive thresholding** for coherence metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### **Notebook Flow**\n",
    "1. **Setup and Initialization**: Import necessary libraries, set up paths, and initialize logging.\n",
    "2. **Data Extraction**: Load and preview the data from the three corpora (2010-2014, 2014-2019, 2020-2024).\n",
    "3. **Text Preprocessing**: Tokenize, filter, and tag data for each paragraph.\n",
    "4. **Batch Formation**: Implement batch formation strategies (balanced sampling, diversity-based clustering).\n",
    "5. **Output Generation**: Save the processed data for further use in topic modeling.\n",
    "\n",
    "**Note**: This notebook is designed to be modular and adaptive, supporting different types of textual data as we continue to refine the topic modeling pipeline.\n",
    "\n",
    "--- \n",
    "\n",
    "**Let’s get started by setting up the environment and extracting the data.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os            # Provides functions for interacting with the operating system, e.g., file handling.\n",
    "import re            # Supports regular expressions for text manipulation and pattern matching.\n",
    "import csv           # Facilitates reading from and writing to CSV files.\n",
    "import json          # Enables reading from and writing to JSON files, often used for structured data.\n",
    "from time import time  # Allows timing operations for performance measurement.\n",
    "import logging       # Provides logging functionalities to monitor code execution.\n",
    "import codecs        # Handles different text encodings, important for text data processing.\n",
    "import multiprocessing  # Supports parallel processing, enhancing performance on large datasets.\n",
    "import pprint as pp  # Pretty-prints data structures, useful for debugging and visualization.\n",
    "from hashlib import md5  # Provides MD5 hashing, useful for generating unique IDs or checking data integrity.\n",
    "\n",
    "# Encoding and Parsing Imports\n",
    "import chardet        # Detects character encoding of text files, allowing for accurate reading of various encodings.\n",
    "import unicodedata    # Handles Unicode character information, useful for identifying and removing non-printable characters.\n",
    "from bs4 import BeautifulSoup  # Parses HTML content, enabling extraction of specific tags (e.g., <p> tags) for processing.\n",
    "\n",
    "\n",
    "# NLTK Imports\n",
    "import nltk                             # Natural Language Toolkit, a suite for text processing.\n",
    "from nltk.corpus import stopwords       # Provides lists of stop words to remove from text.\n",
    "from nltk.corpus.reader.api import CorpusReader  # Base class for reading and structuring corpora.\n",
    "from nltk import sent_tokenize, pos_tag, wordpunct_tokenize  # Tokenizers and POS tagger for sentence processing.\n",
    "stop_words = stopwords.words('english')  # Initializing English stop words list for filtering out common words.\n",
    "\n",
    "# Gensim Imports\n",
    "import gensim                           # Library for topic modeling and word vector creation.\n",
    "from gensim.models import Word2Vec, ldamulticore  # Word2Vec for word embeddings, ldamulticore for topic modeling.\n",
    "from gensim.models.phrases import Phrases, Phraser  # Constructs multi-word phrases (e.g., bigrams) from tokenized text.\n",
    "import gensim.corpora as corpora        # Handles creation of dictionaries and corpora from text data.\n",
    "from gensim.utils import simple_preprocess  # Preprocesses text into a list of tokens.\n",
    "\n",
    "# SpaCy Import (specific model)\n",
    "import en_core_web_lg                   # SpaCy's large English NLP model for advanced text processing.\n",
    "nlp = en_core_web_lg.load(disable=['parser','ner'])  # Loads model, with parsing and named entity recognition disabled.\n",
    "\n",
    "# Readability Import\n",
    "from readability.readability import Unparseable  # Exception handling for parsing errors in HTML.\n",
    "from readability.readability import Document as Paper  # Extracts readable content from HTML, discarding noise.\n",
    "\n",
    "# BeautifulSoup and HTML5lib\n",
    "import bs4                                # BeautifulSoup for parsing HTML and XML documents.\n",
    "import html5lib                           # Parser for HTML5, used by BeautifulSoup for web scraping.\n",
    "\n",
    "# Data Processing and Scientific Libraries\n",
    "import numpy as np                        # Supports efficient numerical operations on large arrays and matrices.\n",
    "import pandas as pd                       # Data analysis library for handling structured data (e.g., DataFrames).\n",
    "from sklearn.manifold import TSNE         # Dimensionality reduction for visualizing high-dimensional data.\n",
    "from matplotlib import pyplot as plt      # Plotting library for creating visualizations.\n",
    "from tqdm import tqdm                     # Adds progress bars to loops, useful for monitoring lengthy operations.\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_ID = r'.*[\\d\\w\\-.]+\\.(html|json)$'  # Regular expression pattern to identify document filenames ending in .html or .json.\n",
    "\n",
    "TAGS = ['p']  # List of HTML tags to extract content from; 'p' is commonly used to denote paragraphs in HTML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DocumentParser(CorpusReader):\n",
    "       \n",
    "    def __init__(self, root, tags=TAGS, fileids=DOC_ID, **kwargs):\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "        self.tags = tags\n",
    "        # Statistics tracking\n",
    "        self.error_character_stats = defaultdict(int)\n",
    "        self.parsing_errors_stats = defaultdict(int)\n",
    "        self.token_frequency_stats = defaultdict(int)\n",
    "        self.foreign_sentence_count = 0\n",
    "        self.mixed_language_sentence_count = 0\n",
    "        self.paragraph_language_counts = defaultdict(int)\n",
    "        self.sentence_length_stats = []\n",
    "        self.paragraph_length_stats = []\n",
    "        self.unique_tokens = set()\n",
    "        self.special_character_usage = defaultdict(int)\n",
    "\n",
    "    def resolve(self, fileids=None):\n",
    "        return fileids \n",
    "\n",
    "    def detect_encoding(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "        return chardet.detect(raw_data)['encoding']\n",
    "    \n",
    "    def process_content(self, content, non_html_log_file):\n",
    "        content = unicodedata.normalize('NFKC', content.replace(\"\\n\", \"\\n\").strip())\n",
    "\n",
    "        try:\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "\n",
    "            for p in paragraphs:\n",
    "                text = p.get_text()\n",
    "                tokenized_paragraph = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "                # Track token frequencies and unique tokens\n",
    "                for token in tokenized_paragraph:\n",
    "                    self.token_frequency_stats[token] += 1\n",
    "                    self.unique_tokens.add(token)\n",
    "\n",
    "                # Track control characters in individual tokens\n",
    "                for token in tokenized_paragraph:\n",
    "                    for char in token:\n",
    "                        if (ord(char) < 32 and char not in '\\n\\t') or unicodedata.category(char) in ['Cc', 'Cf']:\n",
    "                            self.error_character_stats[(f\"U+{ord(char):04X}\", unicodedata.name(char, \"UNKNOWN\"))] += 1\n",
    "\n",
    "                # Count foreign and mixed-language sentences\n",
    "                contains_foreign = any(any(ord(char) > 127 for char in token) for token in tokenized_paragraph)\n",
    "                contains_latin = any(any('LATIN' in unicodedata.name(char, '') for char in token if ord(char) > 127) for token in tokenized_paragraph)\n",
    "\n",
    "                if contains_foreign:\n",
    "                    self.foreign_sentence_count += 1\n",
    "\n",
    "                if contains_foreign and contains_latin:\n",
    "                    self.mixed_language_sentence_count += 1\n",
    "\n",
    "                # Determine predominant language in paragraphs\n",
    "                non_latin_count = sum(1 for token in tokenized_paragraph for char in token if ord(char) > 127)\n",
    "                latin_count = sum(1 for token in tokenized_paragraph for char in token if 'LATIN' in unicodedata.name(char, '') and ord(char) <= 127)\n",
    "\n",
    "                if non_latin_count > latin_count:\n",
    "                    self.paragraph_language_counts['foreign'] += 1\n",
    "                else:\n",
    "                    self.paragraph_language_counts['latin'] += 1\n",
    "\n",
    "                # Track paragraph length\n",
    "                self.paragraph_length_stats.append(len(tokenized_paragraph))\n",
    "\n",
    "                # Track special character usage\n",
    "                for char in text:\n",
    "                    if not char.isalnum() and char not in (' ', '\\n', '\\t'):\n",
    "                        self.special_character_usage[char] += 1\n",
    "\n",
    "                # Remove all control characters except newline (U+000A) and tab (U+0009)\n",
    "                cleaned_text = re.sub(r'[^\\x09\\x0A\\x20-\\x7E\\x80-\\uFFFF]', '', text)\n",
    "                \n",
    "                # Identify and log any remaining control characters (excluding newline and tab)\n",
    "                control_chars = [\n",
    "                    (char, f\"U+{ord(char):04X}\", unicodedata.name(char, \"UNKNOWN\"))\n",
    "                    for char in text if ord(char) < 32 and char not in '\\n\\t'\n",
    "                ]\n",
    "\n",
    "                if control_chars:\n",
    "                    char_details = \"; \".join([f\"{c} ({code}: {name})\" for c, code, name in control_chars])\n",
    "                    non_html_log_file.write(f\"Control characters found in paragraph: {char_details}\\n\")\n",
    "                    \n",
    "                # Track sentence length statistics\n",
    "                sentences = nltk.sent_tokenize(text)\n",
    "                for sentence in sentences:\n",
    "                    self.sentence_length_stats.append(len(re.findall(r'\\b\\w+\\b', sentence)))\n",
    "                \n",
    "                yield cleaned_text\n",
    "        except Exception as e:\n",
    "            self.parsing_errors_stats[str(e)] += 1\n",
    "            non_html_log_file.write(f\"Error processing as HTML: {str(e)}\\n\")\n",
    "            yield None\n",
    "\n",
    "\n",
    "    def docs(self, fileids=None, pattern=None, non_html_log_file=None):\n",
    "        fileids = self.resolve(fileids)\n",
    "        if pattern is not None:\n",
    "            regex = re.compile(pattern, re.IGNORECASE)\n",
    "        else:\n",
    "            regex = re.compile(r'.*([\\d\\w\\-.]+)\\.(html|json)$', re.IGNORECASE)\n",
    "            \n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            if regex.search(path):\n",
    "                encoding = self.detect_encoding(path)\n",
    "                \n",
    "                if path.lower().endswith('.json'):\n",
    "                    with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                        try:\n",
    "                            data = json.load(f)\n",
    "                            if isinstance(data, list):\n",
    "                                for item in data:\n",
    "                                    if isinstance(item, str):\n",
    "                                        for content in self.process_content(item, non_html_log_file):\n",
    "                                            if content:\n",
    "                                                yield self.replace_curly_quotes(content)\n",
    "                            else:\n",
    "                                print(f\"Error: {path} does not contain a list of HTML strings.\")\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(f\"Error: {path} is not a valid JSON file.\")\n",
    "                elif path.lower().endswith('.html'):\n",
    "                    with open(path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                        doc_content = f.read()\n",
    "                        for content in self.process_content(doc_content, non_html_log_file):\n",
    "                            if content:\n",
    "                                yield self.replace_curly_quotes(content)\n",
    "                else:\n",
    "                    print(f\"Unsupported file type: {path}. Only JSON and HTML files are allowed.\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "    def html(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Iterates over HTML documents, yielding content for each document.\n",
    "\n",
    "        Parameters:\n",
    "            fileids (str or None): Specific file identifier(s) or None.\n",
    "\n",
    "        Yields:\n",
    "            str: Parsed content from each HTML document.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids):\n",
    "            try:\n",
    "                yield doc\n",
    "            except Exception as e:\n",
    "                print(\"Could not parse HTML: {}\".format(e))\n",
    "                continue\n",
    "\n",
    "\n",
    "    def replace_curly_quotes(self, text):\n",
    "        \"\"\"\n",
    "        Replaces curly quotes with straight quotes in the provided text.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The text to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The text with curly quotes replaced by straight quotes.\n",
    "        \"\"\"\n",
    "        quote_replacements = {\n",
    "            u\"\\u2018\": \"'\",  # Left single quotation mark\n",
    "            u\"\\u2019\": \"'\",  # Right single quotation mark\n",
    "            u\"\\u201C\": '\"',  # Left double quotation mark\n",
    "            u\"\\u201D\": '\"',  # Right double quotation mark\n",
    "        }\n",
    "        \n",
    "        for curly_quote, straight_quote in quote_replacements.items():\n",
    "            text = text.replace(curly_quote, straight_quote)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def remove_non_printable_chars(self, text):\n",
    "        \"\"\"\n",
    "        Removes non-printable characters from the provided text.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The text to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The text with non-printable characters removed.\n",
    "        \"\"\"\n",
    "        non_printable_pattern = re.compile(r'[\\x00-\\x1F\\x7F-\\x9F]+')\n",
    "        cleaned_text = re.sub(non_printable_pattern, '', text)\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "\n",
    "    def get_invalid_character_names(self, text):\n",
    "        \"\"\"\n",
    "        Retrieves the names of non-printable characters in the text.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The text to analyze.\n",
    "\n",
    "        Returns:\n",
    "            set: A set of names of non-printable characters found in the text.\n",
    "        \"\"\"\n",
    "        char_names = set()\n",
    "        non_printable_pattern = re.compile(r'[\\x00-\\x1F\\x7F-\\x9F]')\n",
    "        invalid_chars = non_printable_pattern.findall(text)\n",
    "        for char in invalid_chars:\n",
    "            try:\n",
    "                name = unicodedata.name(char)\n",
    "            except ValueError:\n",
    "                name = \"UNKNOWN CONTROL CHARACTER\"\n",
    "            char_names.add(name)\n",
    "        return char_names\n",
    "\n",
    "    def paras(self, parser_type='lxml', fileids=None):\n",
    "            \"\"\"\n",
    "            Extracts paragraphs from HTML content based on specified tags.\n",
    "\n",
    "            Parameters:\n",
    "                parser_type (str): Parser type for BeautifulSoup (default is 'lxml').\n",
    "                fileids (str or None): Specific file identifier(s) or None.\n",
    "\n",
    "            Yields:\n",
    "                str: Extracted paragraph text.\n",
    "            \"\"\"\n",
    "            for html in self.html(fileids):\n",
    "                # Check if html content looks like an HTML string\n",
    "                if not isinstance(html, str) or \"<\" not in html:\n",
    "                    print(f\"Skipping non-HTML content: {html}\")\n",
    "                    continue\n",
    "                \n",
    "                soup = BeautifulSoup(html, parser_type)\n",
    "                \n",
    "                # Join tags into a CSS selector if `self.tags` is a list\n",
    "                tag_selector = \",\".join(self.tags) if isinstance(self.tags, list) else self.tags\n",
    "                \n",
    "                for element in soup.select(tag_selector):\n",
    "                    text = element.text.strip()\n",
    "                    yield text\n",
    "\n",
    "    def sents(self, fileids=None):\n",
    "            \"\"\"\n",
    "            Splits paragraphs into sentences.\n",
    "\n",
    "            Parameters:\n",
    "                fileids (str or None): Specific file identifier(s) or None.\n",
    "\n",
    "            Yields:\n",
    "                str: Extracted sentence text.\n",
    "            \"\"\"\n",
    "            for paragraph in self.paras(fileids=fileids):\n",
    "                for sentence in nltk.sent_tokenize(paragraph): \n",
    "                    yield sentence\n",
    "\n",
    "    def words(self, fileids=None): \n",
    "            \"\"\"\n",
    "            Splits sentences into individual words.\n",
    "\n",
    "            Parameters:\n",
    "                fileids (str or None): Specific file identifier(s) or None.\n",
    "\n",
    "            Yields:\n",
    "                str: Extracted word token.\n",
    "            \"\"\"\n",
    "            for sentence in self.sents(fileids=fileids):\n",
    "                for token in nltk.wordpunct_tokenize(sentence):\n",
    "                    yield token\n",
    "                    \n",
    "    def validate_paragraph(self, paragraph):\n",
    "        \"\"\"\n",
    "        Validates a paragraph, checking for non-printable characters and whitespace-only content.\n",
    "\n",
    "        Parameters:\n",
    "            paragraph (str): The paragraph to validate.\n",
    "\n",
    "        Returns:\n",
    "            bool or str: True if valid, otherwise a string explaining the issues.\n",
    "        \"\"\"\n",
    "        reasons = []\n",
    "        if not paragraph.strip():\n",
    "            reasons.append(\"Only whitespace\")\n",
    "\n",
    "        invalid_char_names = self.get_invalid_character_names(paragraph)\n",
    "        \n",
    "        if invalid_char_names:\n",
    "            reason = f\"Contains non-printable characters: {', '.join(invalid_char_names)}\"\n",
    "            reasons.append(reason)\n",
    "\n",
    "        return True if not reasons else ', '.join(reasons)\n",
    "\n",
    "\n",
    "    def get_token_frequency(self):\n",
    "        \"\"\"Returns token frequency statistics.\"\"\"\n",
    "        return {token: count for token, count in self.token_frequency_stats.items()}\n",
    "\n",
    "    def get_error_statistics(self):\n",
    "        \"\"\"Return collected error statistics from the wrapped processing.\"\"\"\n",
    "        character_stats = {f\"{code} ({name})\": count for (code, name), count in self.error_character_stats.items()}\n",
    "        other_error_stats = dict(self.parsing_errors_stats)\n",
    "\n",
    "        return {\n",
    "            \"character_errors\": character_stats,\n",
    "            \"parsing_errors\": other_error_stats,\n",
    "            \"foreign_sentence_count\": self.foreign_sentence_count,\n",
    "            \"mixed_language_sentence_count\": self.mixed_language_sentence_count,\n",
    "            \"paragraph_language_counts\": dict(self.paragraph_language_counts)\n",
    "        }\n",
    "\n",
    "    def process_all_and_collect_stats(self, content, non_html_log_file):\n",
    "        \"\"\"Process entire content and collect stats without changing the original `CorpusReader` methods.\"\"\"\n",
    "        list(self.process_content(self, content, non_html_log_file))  # Process content using the wrapper\n",
    "        # Return error statistics after processing the entire content\n",
    "        return self.get_error_statistics()\n",
    "    \n",
    "    def get_error_statistics(self):\n",
    "        character_stats = {f\"{code} ({name})\": count for (code, name), count in self.error_character_stats.items()}\n",
    "        other_error_stats = dict(self.parsing_errors_stats)\n",
    "        sentence_length_mean = sum(self.sentence_length_stats) / len(self.sentence_length_stats) if self.sentence_length_stats else 0\n",
    "        paragraph_length_mean = sum(self.paragraph_length_stats) / len(self.paragraph_length_stats) if self.paragraph_length_stats else 0\n",
    "        sentence_length_median = statistics.median(self.sentence_length_stats) if self.sentence_length_stats else 0\n",
    "        paragraph_length_median = statistics.median(self.paragraph_length_stats) if self.paragraph_length_stats else 0\n",
    "        sentence_length_mode = statistics.mode(self.sentence_length_stats) if self.sentence_length_stats else 0\n",
    "        paragraph_length_mode = statistics.mode(self.paragraph_length_stats) if self.paragraph_length_stats else 0\n",
    "        sentence_length_stdev = statistics.stdev(self.sentence_length_stats) if len(self.sentence_length_stats) > 1 else 0\n",
    "        paragraph_length_stdev = statistics.stdev(self.paragraph_length_stats) if len(self.paragraph_length_stats) > 1 else 0\n",
    "\n",
    "        return {\n",
    "            \"character_errors\": character_stats,\n",
    "            \"parsing_errors\": other_error_stats,\n",
    "            \"foreign_sentence_count\": self.foreign_sentence_count,\n",
    "            \"mixed_language_sentence_count\": self.mixed_language_sentence_count,\n",
    "            \"paragraph_language_counts\": dict(self.paragraph_language_counts),\n",
    "            \"unique_token_count\": len(self.unique_tokens),\n",
    "            \"special_character_usage\": dict(self.special_character_usage),\n",
    "            \"sentence_length_mean\": sentence_length_mean,\n",
    "            \"paragraph_length_mean\": paragraph_length_mean,\n",
    "            \"sentence_length_median\": sentence_length_median,\n",
    "            \"paragraph_length_median\": paragraph_length_median,\n",
    "            \"sentence_length_mode\": sentence_length_mode,\n",
    "            \"paragraph_length_mode\": paragraph_length_mode,\n",
    "            \"sentence_length_stdev\": sentence_length_stdev,\n",
    "            \"paragraph_length_stdev\": paragraph_length_stdev\n",
    "        }\n",
    "\n",
    "\n",
    "    def write_statistics(self, output_parquet_path, output_csv_path):\n",
    "        \"\"\"Write collected statistics to a Parquet file.\"\"\"\n",
    "        # Collect statistics\n",
    "        error_stats = self.get_error_statistics()\n",
    "        token_frequency = self.get_token_frequency()\n",
    "\n",
    "        # Prepare data for writing to Parquet\n",
    "        data = []\n",
    "\n",
    "        # Add Token Frequency Statistics\n",
    "        for token, count in token_frequency.items():\n",
    "            data.append({'Statistic': f\"Token Frequency: {token}\", 'Value': count})\n",
    "\n",
    "        # Add Character Error Statistics\n",
    "        for char_info, count in error_stats['character_errors'].items():\n",
    "            data.append({'Statistic': f\"Character Error: {char_info}\", 'Value': count})\n",
    "\n",
    "        # Add Parsing Errors\n",
    "        for error, count in error_stats['parsing_errors'].items():\n",
    "            data.append({'Statistic': f\"Parsing Error: {error}\", 'Value': count})\n",
    "\n",
    "        # Add Foreign and Mixed Language Sentence Counts\n",
    "        data.append({'Statistic': \"Foreign Sentence Count\", 'Value': error_stats['foreign_sentence_count']})\n",
    "        data.append({'Statistic': \"Mixed Language Sentence Count\", 'Value': error_stats['mixed_language_sentence_count']})\n",
    "\n",
    "        # Add Paragraph Language Counts\n",
    "        for lang, count in error_stats['paragraph_language_counts'].items():\n",
    "            data.append({'Statistic': f\"Paragraph Language Count: {lang}\", 'Value': count})\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Write DataFrame to Parquet file\n",
    "        df.to_parquet(output_parquet_path, index=False)\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "\n",
    "    def generate(self, fileids=None, log_file_path=None, non_html_log_path=None):\n",
    "        \"\"\"\n",
    "        Processes documents, logging invalid paragraphs and returning valid ones.\n",
    "\n",
    "        Parameters:\n",
    "            fileids (str or None): Specific file identifier(s) or None.\n",
    "            log_file_path (str): Path for logging invalid paragraphs.\n",
    "            non_html_log_path (str): Path for logging non-HTML content.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - doc_dict (list): List of valid paragraphs.\n",
    "                - error_dict (list): List of invalid paragraphs.\n",
    "                - count (int): Number of valid paragraphs added after cleaning.\n",
    "                - all_paragraph_count (int): Total number of valid paragraphs.\n",
    "        \"\"\"\n",
    "        doc_dict = []\n",
    "        error_dict = []\n",
    "        count = 0\n",
    "        all_paragraph_count = 0 \n",
    "\n",
    "        # Open two log files: one for invalid paragraphs and one for non-HTML content\n",
    "        with open(log_file_path, 'a') as invalid_log_file, open(non_html_log_path, 'a') as non_html_log_file:\n",
    "            for idx, html_content in enumerate(self.docs(fileids=fileids, non_html_log_file=non_html_log_file)):\n",
    "                html_content = self.replace_curly_quotes(html_content)\n",
    "                validation_result = self.validate_paragraph(html_content)\n",
    "\n",
    "                if isinstance(validation_result, bool) and validation_result:  \n",
    "                    # Valid paragraph\n",
    "                    all_paragraph_count += 1\n",
    "                    doc_dict.append(html_content)\n",
    "                else:\n",
    "                    # Invalid paragraph; log to the invalid paragraphs file\n",
    "                    if not isinstance(validation_result, bool):\n",
    "                        invalid_log_file.write(f\"Invalid Paragraph {count}: {validation_result}\\n\")\n",
    "                        cleaned_html_content = self.remove_non_printable_chars(html_content)\n",
    "\n",
    "                        if isinstance(self.validate_paragraph(cleaned_html_content), bool):\n",
    "                            count += 1\n",
    "                            doc_dict.append(cleaned_html_content)\n",
    "                        else:\n",
    "                            error_dict.append(cleaned_html_content)\n",
    "\n",
    "        return doc_dict, self.get_error_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2024_html.json']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus_path = os.path.join(\"topic-modeling\", \"data\", \"docs-to-process\", \"PROJECT_FOLDER\")\n",
    "corpus_path = r\"C:\\utma\\data\\docs-to-process\\2024\"\n",
    "\n",
    "_corpus = DocumentParser(corpus_path)\n",
    "# print filenames\n",
    "_corpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generic paths for log files\n",
    "base_path = r\"C:\\utma\\data\\docs-to-process\\2024\\log\"\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "log_file_path = os.path.join(base_path, \"paragraph_error.log\")\n",
    "non_html_log_path = os.path.join(base_path, \"non_html_content.log\")\n",
    "\n",
    "# Run the generate function with generic paths\n",
    "corpus_tuple, get_error_statistics = _corpus.generate(\n",
    "    log_file_path=log_file_path,\n",
    "    non_html_log_path=non_html_log_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'character_errors': {},\n",
      " 'foreign_sentence_count': 0,\n",
      " 'mixed_language_sentence_count': 0,\n",
      " 'paragraph_language_counts': {'latin': 260},\n",
      " 'paragraph_length_mean': 50.01538461538462,\n",
      " 'paragraph_length_median': 22.0,\n",
      " 'paragraph_length_mode': 1,\n",
      " 'paragraph_length_stdev': 65.257078151617,\n",
      " 'parsing_errors': {},\n",
      " 'sentence_length_mean': 20.906752411575564,\n",
      " 'sentence_length_median': 16.0,\n",
      " 'sentence_length_mode': 1,\n",
      " 'sentence_length_stdev': 28.420598539092097,\n",
      " 'special_character_usage': {'#': 4,\n",
      "                             '%': 26,\n",
      "                             '&': 2,\n",
      "                             '(': 206,\n",
      "                             ')': 212,\n",
      "                             '*': 58,\n",
      "                             ',': 986,\n",
      "                             '-': 246,\n",
      "                             '.': 1008,\n",
      "                             '/': 250,\n",
      "                             ':': 94,\n",
      "                             ';': 310,\n",
      "                             '<': 4,\n",
      "                             '=': 38,\n",
      "                             '?': 8,\n",
      "                             '@': 16,\n",
      "                             '[': 4,\n",
      "                             ']': 4,\n",
      "                             '_': 24,\n",
      "                             '§': 38,\n",
      "                             '°': 8,\n",
      "                             '¶': 30,\n",
      "                             '–': 64,\n",
      "                             '—': 8,\n",
      "                             '’': 22,\n",
      "                             '“': 2,\n",
      "                             '”': 2,\n",
      "                             '†': 42,\n",
      "                             '≥': 4},\n",
      " 'unique_token_count': 1959}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(get_error_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to C:\\utma\\data\\GrandUnifiedProject\\statistics\\2024_stats.JSON\n"
     ]
    }
   ],
   "source": [
    "output_JSON_path = r'C:\\utma\\data\\GrandUnifiedProject\\statistics\\2024_stats.JSON'\n",
    "\n",
    "# Convert nested dictionaries to JSON strings\n",
    "data_for_JSON = {key: (json.dumps(value) if isinstance(value, dict) else value)\n",
    "                for key, value in get_error_statistics.items()}\n",
    "\n",
    "with open(output_JSON_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    def write_nested_dict(d, parent_key=\"\"):\n",
    "        for key, value in d.items():\n",
    "            if isinstance(value, dict):  # Nested dictionary\n",
    "                write_nested_dict(value, parent_key=f\"{parent_key}{key}.\")\n",
    "            else:\n",
    "                writer.writerow([f\"{parent_key}{key}\", value])\n",
    "    \n",
    "    write_nested_dict(get_error_statistics)\n",
    "\n",
    "print(f\"Data written to {output_JSON_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'character_errors': {},\n",
      " 'foreign_sentence_count': 0,\n",
      " 'mixed_language_sentence_count': 0,\n",
      " 'paragraph_language_counts': {},\n",
      " 'paragraph_length_mean': 0,\n",
      " 'paragraph_length_median': 0,\n",
      " 'paragraph_length_mode': 0,\n",
      " 'paragraph_length_stdev': 0,\n",
      " 'parsing_errors': {},\n",
      " 'sentence_length_mean': 0,\n",
      " 'sentence_length_median': 0,\n",
      " 'sentence_length_mode': 0,\n",
      " 'sentence_length_stdev': 0,\n",
      " 'special_character_usage': {},\n",
      " 'unique_token_count': 0}\n"
     ]
    }
   ],
   "source": [
    "output_parquet_path = r'C:\\utma\\data\\GrandUnifiedProject\\statistics\\2010_2014_stats.parquet'\n",
    "output_csv_path = r'C:\\utma\\data\\GrandUnifiedProject\\statistics\\2010_2014_stats.csv'\n",
    "\n",
    "\n",
    "_corpus.write_statistics(output_parquet_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set flag to determine if lemmatization (reducing words to their base form) will be performed during preprocessing.\n",
    "INCLUDE_STOPWORDS = True\n",
    "LEMMATIZATION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_out = []       # List to store processed text (filtered and lemmatized tokens) for each paragraph\n",
    "inner_text = []      # Temporary list to hold tokens for the current paragraph\n",
    "\n",
    "# Revised processing to retain all parts of speech and keep stop words\n",
    "for paras in tqdm(corpus_tuple, total=len(corpus_tuple)):\n",
    "    doc = nlp(paras)  # Process the paragraph with spaCy NLP pipeline\n",
    "\n",
    "    for token in doc:\n",
    "        # Consider all tokens, not just content words, and reduce character length threshold if necessary\n",
    "        if len(token.text) > 1:  # Example threshold adjustment to keep even short words if relevant\n",
    "            # Optionally include or remove stop words\n",
    "            if INCLUDE_STOPWORDS or (token.text.lower() not in stop_words and token.lemma_.lower() not in stop_words):\n",
    "                inner_text.append(token.text if not LEMMATIZATION else token.lemma_)\n",
    "\n",
    "# Append processed tokens of the current paragraph to texts_out if not empty\n",
    "if len(inner_text) > 0:\n",
    "    texts_out.append(inner_text)\n",
    "inner_text = []  # Reset inner_text for the next paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
