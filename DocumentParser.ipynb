{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpectraSync: Neural Intelligence Meets Multi-Dimensional Topic Analysis\n",
    "\n",
    "This notebook performs document preprocessing for the SpectraSync: Neural Intelligence Meets Multi-Dimensional Topic Analysis pipeline, ensuring documents are prepared for seamless ingestion by `specrasync.py` and related scripts. The SpectraSync pipeline integrates topic modeling, diachronic analysis, and visualization, allowing for adaptable and detailed analysis across diverse textual corpora. This notebook ensures document standardization, facilitating compatibility with SpectraSync's topic modeling and evaluation stages.\n",
    "\n",
    "### Notebook Objectives\n",
    "- **Load and Transform Documents**: Imports and structures text from HTML and JSON files, preparing them for SpectraSync's topic analysis.\n",
    "- **Data Cleansing**: Standardizes text by removing curly quotes, non-printable characters, and other inconsistencies.\n",
    "- **Content Structuring**: Extracts text using BeautifulSoup and regex, arranging it into paragraphs and sentences.\n",
    "- **Output Preparation**: Produces a processed corpus in a format optimized for `SpectraSync.py` ingestion, supporting downstream analysis and visualization.\n",
    "\n",
    "### Dependencies and Related Components\n",
    "This notebook is designed to work alongside the following scripts:\n",
    "- **`SpectraSync.py`**: Coordinates topic modeling, diachronic analysis, and evaluation.\n",
    "- **`alpha_eta.py`**: Supports hyperparameter tuning for model optimization.\n",
    "- **`process_futures.py`**: Manages asynchronous processing, essential for handling large datasets efficiently.\n",
    "- **`topic_model_trainer.py`**: Defines and trains the topic models used in SpectraSync.\n",
    "- **`visualization.py`**: Generates visualizations for model insights and evaluation.\n",
    "- **`write_to_postgres.py`**: Facilitates data persistence into PostgreSQL, supporting structured data retrieval.\n",
    "- **`utils.py`**: Provides utility functions to enhance efficiency and consistency across the SpectraSync pipeline.\n",
    "\n",
    "### Workflow Overview\n",
    "1. **Document Loading**: Reads HTML or JSON files, detecting encoding where necessary.\n",
    "2. **Content Extraction**: Extracts structured content, normalizing punctuation and replacing curly quotes for text consistency.\n",
    "3. **Data Output**: Saves the processed content in a structured format for direct use in `spectrasync.py`.\n",
    "\n",
    "Running this notebook prepares a clean, standardized corpus compatible with the SpectraSync pipeline, optimizing input quality for topic modeling and diachronic analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os            # Provides functions for interacting with the operating system, e.g., file handling.\n",
    "import re            # Supports regular expressions for text manipulation and pattern matching.\n",
    "import csv           # Facilitates reading from and writing to CSV files.\n",
    "import json          # Enables reading from and writing to JSON files, often used for structured data.\n",
    "from time import time  # Allows timing operations for performance measurement.\n",
    "import logging       # Provides logging functionalities to monitor code execution.\n",
    "import codecs        # Handles different text encodings, important for text data processing.\n",
    "import multiprocessing  # Supports parallel processing, enhancing performance on large datasets.\n",
    "import pprint as pp  # Pretty-prints data structures, useful for debugging and visualization.\n",
    "from hashlib import md5  # Provides MD5 hashing, useful for generating unique IDs or checking data integrity.\n",
    "\n",
    "# Encoding and Parsing Imports\n",
    "import chardet        # Detects character encoding of text files, allowing for accurate reading of various encodings.\n",
    "import unicodedata    # Handles Unicode character information, useful for identifying and removing non-printable characters.\n",
    "from bs4 import BeautifulSoup  # Parses HTML content, enabling extraction of specific tags (e.g., <p> tags) for processing.\n",
    "\n",
    "\n",
    "# NLTK Imports\n",
    "import nltk                             # Natural Language Toolkit, a suite for text processing.\n",
    "from nltk.corpus import stopwords       # Provides lists of stop words to remove from text.\n",
    "from nltk.corpus.reader.api import CorpusReader  # Base class for reading and structuring corpora.\n",
    "from nltk import sent_tokenize, pos_tag, wordpunct_tokenize  # Tokenizers and POS tagger for sentence processing.\n",
    "stop_words = stopwords.words('english')  # Initializing English stop words list for filtering out common words.\n",
    "\n",
    "# Gensim Imports\n",
    "import gensim                           # Library for topic modeling and word vector creation.\n",
    "from gensim.models import Word2Vec, ldamulticore  # Word2Vec for word embeddings, ldamulticore for topic modeling.\n",
    "from gensim.models.phrases import Phrases, Phraser  # Constructs multi-word phrases (e.g., bigrams) from tokenized text.\n",
    "import gensim.corpora as corpora        # Handles creation of dictionaries and corpora from text data.\n",
    "from gensim.utils import simple_preprocess  # Preprocesses text into a list of tokens.\n",
    "\n",
    "# SpaCy Import (specific model)\n",
    "import en_core_web_lg                   # SpaCy's large English NLP model for advanced text processing.\n",
    "nlp = en_core_web_lg.load(disable=['parser','ner'])  # Loads model, with parsing and named entity recognition disabled.\n",
    "\n",
    "# Readability Import\n",
    "from readability.readability import Unparseable  # Exception handling for parsing errors in HTML.\n",
    "from readability.readability import Document as Paper  # Extracts readable content from HTML, discarding noise.\n",
    "\n",
    "# BeautifulSoup and HTML5lib\n",
    "import bs4                                # BeautifulSoup for parsing HTML and XML documents.\n",
    "import html5lib                           # Parser for HTML5, used by BeautifulSoup for web scraping.\n",
    "\n",
    "# Data Processing and Scientific Libraries\n",
    "import numpy as np                        # Supports efficient numerical operations on large arrays and matrices.\n",
    "import pandas as pd                       # Data analysis library for handling structured data (e.g., DataFrames).\n",
    "from sklearn.manifold import TSNE         # Dimensionality reduction for visualizing high-dimensional data.\n",
    "from matplotlib import pyplot as plt      # Plotting library for creating visualizations.\n",
    "from tqdm import tqdm                     # Adds progress bars to loops, useful for monitoring lengthy operations.\n",
    "\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom stop words based on document context\n",
    "with open('config/custom_stopwords.json', 'r') as file:\n",
    "    custom_stopwords = json.load(file)\n",
    "\n",
    "# Add CDC MMWR-specific stop words, or any additional terms the user finds appropriate\n",
    "# This approach enables the user to include context-specific stop words, not limited to MMWR.\n",
    "# Users can add any terms they consider irrelevant or noise within the custom_stopwords.json file, \n",
    "# making this process adaptable to various document types or specific text analysis needs.\n",
    "stop_words.extend(custom_stopwords.get(\"cdc_mmwr\", []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_ID = r'.*[\\d\\w\\-.]+\\.(html|json)$'  # Regular expression pattern to identify document filenames ending in .html or .json.\n",
    "\n",
    "TAGS = ['p']  # List of HTML tags to extract content from; 'p' is commonly used to denote paragraphs in HTML.\n",
    "\n",
    "# Set flag to determine if lemmatization (reducing words to their base form) will be performed during preprocessing.\n",
    "LEMMATIZATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DocumentParser(CorpusReader):\n",
    "    \n",
    "    def __init__(self, root, tags=TAGS, fileids=DOC_ID, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the HTMLReader with the root directory, specified tags, and file identifiers.\n",
    "\n",
    "        Parameters:\n",
    "            root (str): The root directory containing the corpus.\n",
    "            tags (list): List of HTML tags to extract (e.g., 'p' for paragraphs).\n",
    "            fileids (str): Regular expression pattern to match file names.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "        self.tags = tags\n",
    "\n",
    "    def resolve(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Resolves the file identifiers, returning them as-is.\n",
    "\n",
    "        Parameters:\n",
    "            fileids (str or None): Specific file identifier(s) or None.\n",
    "\n",
    "        Returns:\n",
    "            str or None: The resolved file identifiers.\n",
    "        \"\"\"\n",
    "        return fileids \n",
    "\n",
    "    def detect_encoding(self, file_path):\n",
    "        \"\"\"\n",
    "        Detects the character encoding of a file.\n",
    "\n",
    "        Parameters:\n",
    "            file_path (str): Path to the file whose encoding needs to be detected.\n",
    "\n",
    "        Returns:\n",
    "            str: The detected encoding of the file.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "        return chardet.detect(raw_data)['encoding']\n",
    "    \n",
    "\n",
    "    def process_content(self, content, non_html_log_file):\n",
    "        \"\"\"\n",
    "        Processes HTML content by extracting text within <p> tags, allowing common punctuation and whitespace,\n",
    "        and removing control characters except newlines and tabs. Logs any remaining control characters for review.\n",
    "\n",
    "        Parameters:\n",
    "            content (str): The raw HTML content to process.\n",
    "            non_html_log_file (file): File handle to log non-HTML content or paragraphs with control characters.\n",
    "\n",
    "        Yields:\n",
    "            str or None: Cleaned text extracted from <p> tags, or None if an error occurs.\n",
    "        \"\"\"\n",
    "        # Normalize Unicode to a standard form (NFKC) to handle variations in character representations\n",
    "        content = unicodedata.normalize('NFKC', content.replace(\"\\\\n\", \"\\n\").strip())\n",
    "\n",
    "        try:\n",
    "            # Parse the HTML content and find all <p> elements\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "\n",
    "            for p in paragraphs:\n",
    "                # Extract text from each <p> element\n",
    "                text = p.get_text()\n",
    "\n",
    "                # Remove all control characters except newline (U+000A) and tab (U+0009)\n",
    "                cleaned_text = re.sub(r'[^\\x09\\x0A\\x20-\\x7E\\x80-\\uFFFF]', '', text)\n",
    "                \n",
    "                # Identify and log any remaining control characters (excluding newline and tab)\n",
    "                control_chars = [\n",
    "                    (char, f\"U+{ord(char):04X}\", unicodedata.name(char, \"UNKNOWN\"))\n",
    "                    for char in text if ord(char) < 32 and char not in '\\n\\t'\n",
    "                ]\n",
    "\n",
    "                # If control characters are found, log their details for review\n",
    "                if control_chars:\n",
    "                    char_details = \"; \".join([f\"{c} ({code}: {name})\" for c, code, name in control_chars])\n",
    "                    non_html_log_file.write(f\"Control characters found in paragraph: {char_details}\\n\")\n",
    "                    \n",
    "                # Yield the cleaned text for further processing\n",
    "                yield cleaned_text\n",
    "        except Exception as e:\n",
    "            # Log any parsing errors and yield None to indicate a failure in processing\n",
    "            non_html_log_file.write(f\"Error processing as HTML: {str(e)}\\n\")\n",
    "            yield None\n",
    "\n",
    "\n",
    "    def docs(self, fileids=None, pattern=None, non_html_log_file=None):\n",
    "        \"\"\"\n",
    "        Iterates over the documents in the corpus, yielding processed content.\n",
    "\n",
    "        Parameters:\n",
    "            fileids (str or None): Specific file identifier(s) or None.\n",
    "            pattern (str or None): Regular expression pattern to filter file names.\n",
    "            non_html_log_file (file or None): File handle to log non-HTML content.\n",
    "\n",
    "        Yields:\n",
    "            str: Processed content from HTML or JSON files.\n",
    "        \"\"\"        \n",
    "        fileids = self.resolve(fileids)\n",
    "        \n",
    "        # Compile the regular expression pattern, or use a default if none is provided\n",
    "        if pattern is not None:\n",
    "            regex = re.compile(pattern, re.IGNORECASE)\n",
    "        else:\n",
    "            regex = re.compile(r'.*([\\d\\w\\-.]+)\\.(html|json)$', re.IGNORECASE)\n",
    "            \n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            if regex.search(path):\n",
    "                encoding = self.detect_encoding(path)\n",
    "                \n",
    "                # Check if the file is JSON by extension\n",
    "                if path.lower().endswith('.json'):\n",
    "                    with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                        try:\n",
    "                            # Load JSON content\n",
    "                            data = json.load(f)\n",
    "                            # Process each HTML snippet in JSON directly for <p> tags\n",
    "                            if isinstance(data, list):\n",
    "                                for item in data:\n",
    "                                    if isinstance(item, str):\n",
    "                                        # Process each content item with HTML or plain text\n",
    "                                        for content in self.process_content(item, non_html_log_file):\n",
    "                                            if content:\n",
    "                                                yield self.replace_curly_quotes(content)\n",
    "                            else:\n",
    "                                print(f\"Error: {path} does not contain a list of HTML strings.\")\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(f\"Error: {path} is not a valid JSON file.\")\n",
    "                \n",
    "                elif path.lower().endswith('.html'):\n",
    "                    # Process as regular HTML file\n",
    "                    with open(path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                        doc_content = f.read()\n",
    "                        # Process as HTML or plain text\n",
    "                        for content in self.process_content(doc_content, non_html_log_file):\n",
    "                            if content:\n",
    "                                yield self.replace_curly_quotes(content)\n",
    "                else:\n",
    "                    print(f\"Unsupported file type: {path}. Only JSON and HTML files are allowed.\")\n",
    "                    continue\n",
    "\n",
    "    def html(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Iterates over HTML documents, yielding content for each document.\n",
    "\n",
    "        Parameters:\n",
    "            fileids (str or None): Specific file identifier(s) or None.\n",
    "\n",
    "        Yields:\n",
    "            str: Parsed content from each HTML document.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids):\n",
    "            try:\n",
    "                yield doc\n",
    "            except Exception as e:\n",
    "                print(\"Could not parse HTML: {}\".format(e))\n",
    "                continue\n",
    "\n",
    "    def replace_curly_quotes(self, text):\n",
    "        \"\"\"\n",
    "        Replaces curly quotes with straight quotes in the provided text.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The text to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The text with curly quotes replaced by straight quotes.\n",
    "        \"\"\"\n",
    "        quote_replacements = {\n",
    "            u\"\\\\u2018\": \"'\",  # Left single quotation mark\n",
    "            u\"\\\\u2019\": \"'\",  # Right single quotation mark\n",
    "            u\"\\\\u201C\": '\"',  # Left double quotation mark\n",
    "            u\"\\\\u201D\": '\"',  # Right double quotation mark\n",
    "        }\n",
    "        \n",
    "        for curly_quote, straight_quote in quote_replacements.items():\n",
    "            text = text.replace(curly_quote, straight_quote)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def remove_non_printable_chars(self, text):\n",
    "        \"\"\"\n",
    "        Removes non-printable characters from the provided text.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The text to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The text with non-printable characters removed.\n",
    "        \"\"\"\n",
    "        non_printable_pattern = re.compile(r'[\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]+')\n",
    "        cleaned_text = re.sub(non_printable_pattern, '', text)\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "    def get_invalid_character_names(self, text):\n",
    "        \"\"\"\n",
    "        Retrieves the names of non-printable characters in the text.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The text to analyze.\n",
    "\n",
    "        Returns:\n",
    "            set: A set of names of non-printable characters found in the text.\n",
    "        \"\"\"\n",
    "        char_names = set()\n",
    "        non_printable_pattern = re.compile(r'[\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]')\n",
    "        invalid_chars = non_printable_pattern.findall(text)\n",
    "        for char in invalid_chars:\n",
    "            try:\n",
    "                name = unicodedata.name(char)\n",
    "            except ValueError:\n",
    "                name = \"UNKNOWN CONTROL CHARACTER\"\n",
    "            char_names.add(name)\n",
    "        return char_names\n",
    "        \n",
    "    def validate_paragraph(self, paragraph):\n",
    "        \"\"\"\n",
    "        Validates a paragraph, checking for non-printable characters and whitespace-only content.\n",
    "\n",
    "        Parameters:\n",
    "            paragraph (str): The paragraph to validate.\n",
    "\n",
    "        Returns:\n",
    "            bool or str: True if valid, otherwise a string explaining the issues.\n",
    "        \"\"\"\n",
    "        reasons = []\n",
    "        if not paragraph.strip():\n",
    "            reasons.append(\"Only whitespace\")\n",
    "\n",
    "        invalid_char_names = self.get_invalid_character_names(paragraph)\n",
    "        \n",
    "        if invalid_char_names:\n",
    "            reason = f\"Contains non-printable characters: {', '.join(invalid_char_names)}\"\n",
    "            reasons.append(reason)\n",
    "\n",
    "        return True if not reasons else ', '.join(reasons)\n",
    "    \n",
    "    para_dict = dict()\n",
    "    def paras(self, parser_type='lxml', fileids=None):\n",
    "        \"\"\"\n",
    "        Extracts paragraphs from HTML content based on specified tags.\n",
    "\n",
    "        Parameters:\n",
    "            parser_type (str): Parser type for BeautifulSoup (default is 'lxml').\n",
    "            fileids (str or None): Specific file identifier(s) or None.\n",
    "\n",
    "        Yields:\n",
    "            str: Extracted paragraph text.\n",
    "        \"\"\"\n",
    "        for html in self.html(fileids):\n",
    "            # Check if html content looks like an HTML string\n",
    "            if not isinstance(html, str) or \"<\" not in html:\n",
    "                print(f\"Skipping non-HTML content: {html}\")\n",
    "                continue\n",
    "            \n",
    "            soup = BeautifulSoup(html, parser_type)\n",
    "            \n",
    "            # Join tags into a CSS selector if `self.tags` is a list\n",
    "            tag_selector = \",\".join(self.tags) if isinstance(self.tags, list) else self.tags\n",
    "            \n",
    "            for element in soup.select(tag_selector):\n",
    "                text = element.text.strip()\n",
    "                yield text\n",
    "\n",
    "    sent_dict = dict()\n",
    "    def sents(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Splits paragraphs into sentences.\n",
    "\n",
    "        Parameters:\n",
    "            fileids (str or None): Specific file identifier(s) or None.\n",
    "\n",
    "        Yields:\n",
    "            str: Extracted sentence text.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids=fileids):\n",
    "            for sentence in nltk.sent_tokenize(paragraph): \n",
    "                yield sentence\n",
    "\n",
    "    word_dict = dict()\n",
    "    def words(self, fileids=None): \n",
    "        \"\"\"\n",
    "        Splits sentences into individual words.\n",
    "\n",
    "        Parameters:\n",
    "            fileids (str or None): Specific file identifier(s) or None.\n",
    "\n",
    "        Yields:\n",
    "            str: Extracted word token.\n",
    "        \"\"\"\n",
    "        for sentence in self.sents(fileids=fileids):\n",
    "            for token in nltk.wordpunct_tokenize(sentence):\n",
    "                yield token\n",
    "\n",
    "    def generate(self, fileids=None, log_file_path=None, non_html_log_path=None):\n",
    "        \"\"\"\n",
    "        Processes documents, logging invalid paragraphs and returning valid ones.\n",
    "\n",
    "        Parameters:\n",
    "            fileids (str or None): Specific file identifier(s) or None.\n",
    "            log_file_path (str): Path for logging invalid paragraphs.\n",
    "            non_html_log_path (str): Path for logging non-HTML content.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - doc_dict (list): List of valid paragraphs.\n",
    "                - error_dict (list): List of invalid paragraphs.\n",
    "                - count (int): Number of valid paragraphs added after cleaning.\n",
    "                - all_paragraph_count (int): Total number of valid paragraphs.\n",
    "        \"\"\"\n",
    "        doc_dict = []\n",
    "        error_dict = []\n",
    "        count = 0\n",
    "        all_paragraph_count = 0 \n",
    "\n",
    "        # Open two log files: one for invalid paragraphs and one for non-HTML content\n",
    "        with open(log_file_path, 'a') as invalid_log_file, open(non_html_log_path, 'a') as non_html_log_file:\n",
    "            for idx, html_content in enumerate(self.docs(fileids=fileids, non_html_log_file=non_html_log_file)):\n",
    "                html_content = self.replace_curly_quotes(html_content)\n",
    "                validation_result = self.validate_paragraph(html_content)\n",
    "\n",
    "                if isinstance(validation_result, bool) and validation_result:  \n",
    "                    # Valid paragraph\n",
    "                    all_paragraph_count += 1\n",
    "                    doc_dict.append(html_content)\n",
    "                else:\n",
    "                    # Invalid paragraph; log to the invalid paragraphs file\n",
    "                    if not isinstance(validation_result, bool):\n",
    "                        invalid_log_file.write(f\"Invalid Paragraph {count}: {validation_result}\\n\")\n",
    "                        cleaned_html_content = self.remove_non_printable_chars(html_content)\n",
    "\n",
    "                        if isinstance(self.validate_paragraph(cleaned_html_content), bool):\n",
    "                            count += 1\n",
    "                            doc_dict.append(cleaned_html_content)\n",
    "                        else:\n",
    "                            error_dict.append(cleaned_html_content)\n",
    "\n",
    "        return doc_dict, error_dict, count, all_paragraph_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Important:** Verify File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_path = os.path.join(\"topic-modeling\", \"data\", \"docs-to-process\", \"PROJECT_FOLDER\")\n",
    "corpus_path = \"/SpectraSync/data/docs-to-process/\"\n",
    "\n",
    "_corpus = DocumentParser(corpus_path)\n",
    "# print filenames\n",
    "_corpus.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Important:** Verify file output location and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generic paths for log files\n",
    "base_path = \"/SpectraSync/data/docs-to-process/\"\n",
    "\n",
    "log_file_path = os.path.join(base_path, \"paragraph_error.log\")\n",
    "non_html_log_path = os.path.join(base_path, \"non_html_content.log\")\n",
    "\n",
    "# Run the generate function with generic paths\n",
    "corpus_tuple, errors, count, all_paragraph_count = _corpus.generate(\n",
    "    log_file_path=log_file_path,\n",
    "    non_html_log_path=non_html_log_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display each error found, indexed for easy reference\n",
    "if errors:\n",
    "    print(\"Errors found during document processing:\")\n",
    "    for i, txt in enumerate(errors, start=1):\n",
    "        if len(txt) == 0: txt = \"NON-PRINTABLE CHARACTER\"\n",
    "        print(f\"Error {i}: {txt}\")\n",
    "else:\n",
    "    print(\"No errors found during document processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import spacy\n",
    "\n",
    "texts_out = []       # List to store processed text (filtered and lemmatized tokens) for each paragraph\n",
    "inner_text = []      # Temporary list to hold tokens for the current paragraph\n",
    "\n",
    "# Frequency distribution to count occurrences of stop words found in the text\n",
    "stopword_count = nltk.FreqDist()\n",
    "\n",
    "# Print message indicating whether lemmatization is enabled\n",
    "pp.pprint(f\"Executing POS/LEMMATIZATION({LEMMATIZATION})\")\n",
    "\n",
    "t = time()  # Start timer to measure processing time\n",
    "\n",
    "# Process each paragraph in corpus_tuple with progress tracking\n",
    "for paras in tqdm(corpus_tuple):\n",
    "    doc = nlp(paras)  # Process the paragraph with spaCy NLP pipeline\n",
    "\n",
    "    for token in doc:\n",
    "        # Check if token is a content word (noun, adjective, verb, or adverb) and has 5+ characters\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']:\n",
    "            # Include only words with a minimum length (e.g., 5 characters) for more meaningful content;\n",
    "            # this threshold can be adjusted as needed by the user.\n",
    "            if len(token.text) >= 5:\n",
    "                # Filter out tokens that are stop words (based on both text and lemma forms)\n",
    "                if token.text.lower() not in stop_words and token.lemma_.lower() not in stop_words:\n",
    "                    # Append token (text or lemma based on LEMMATIZATION setting) to inner_text\n",
    "                    inner_text.append(token.text if not LEMMATIZATION else token.lemma_)\n",
    "                else:\n",
    "                    # If token is a stop word, increment its count in stopword_count\n",
    "                    stopword_count[token.text if not LEMMATIZATION else token.lemma_] += 1\n",
    "\n",
    "    # Append processed tokens of the current paragraph to texts_out if not empty\n",
    "    if len(inner_text) > 0:\n",
    "        texts_out.append(inner_text)\n",
    "    inner_text = []  # Reset inner_text for the next paragraph\n",
    "\n",
    "# Print the time taken to complete the spaCy filtering and processing\n",
    "pp.pprint('Time to finish spaCy filter: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter sentences to include only those with 5 or more words, ensuring more substantial content.\n",
    "texts_for_processing = [sent for sent in texts_out if len(sent) >= 5]\n",
    "\n",
    "# Clean up the texts_out variable as it's no longer needed\n",
    "del texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For QA: Print the number of filtered sentences and display each sentence\n",
    "print(len(texts_for_processing))  # Output the count of sentences meeting the length requirement\n",
    "for text in texts_for_processing:\n",
    "    print(text)  # Display each selected sentence for verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Important:** Verify file output location and filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  # Import JSON module for encoding data as JSON strings\n",
    "\n",
    "def write_tokenized_sentences_to_jsonl(sentences, output_file):\n",
    "    \"\"\"\n",
    "    Writes a list of tokenized sentences to a JSON Lines (.jsonl) file.\n",
    "\n",
    "    Parameters:\n",
    "        sentences (list): A list of tokenized sentences, where each sentence is a list of words.\n",
    "        output_file (str): Path to the output .jsonl file.\n",
    "    \"\"\"\n",
    "    # Open the specified output file in write mode with UTF-8 encoding\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            # Convert each sentence to a JSON string and write it to a new line in the file\n",
    "            json_line = json.dumps(sentence, ensure_ascii=False)\n",
    "            f.write(json_line + '\\n')  # Write each JSON string to a new line\n",
    "\n",
    "# Call the function to save tokenized sentences to a JSONL file, replacing the path as needed\n",
    "write_tokenized_sentences_to_jsonl(texts_for_processing, '/SpectraSync/data/processed-docs/data.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Important:** Verify file output location and filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for saving processed text in JSON format\n",
    "filename2 = \"SpectraSync/data/processed-docs/data.json\"\n",
    "\n",
    "# Open the specified file in write mode\n",
    "with open(filename2, 'w', encoding='utf-8') as jsonfile:\n",
    "    # Write the processed text data to the JSON file with formatting and UTF-8 encoding\n",
    "    json.dump(texts_for_processing, jsonfile, indent=1, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gensim's Phrases for detecting common word pairs (bigrams) and triplets (trigrams)\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Create bigrams (two-word combinations) from the processed text data in `texts_for_processing`.\n",
    "# Only word pairs that appear frequently (default threshold of 20 times or more) are considered valid bigrams.\n",
    "bigram = Phrases(texts_for_processing, min_count=20)\n",
    "\n",
    "# Initialize a frequency distribution object to count occurrences of each bigram for analysis\n",
    "bigram_freq = nltk.FreqDist()\n",
    "\n",
    "# Display detected bigrams for review; this allows verification of commonly identified phrases.\n",
    "for ngrams, _ in bigram.vocab.items():\n",
    "    if '_' in ngrams:  # Identify only bigrams (contains '_')\n",
    "        bigram_freq[ngrams] += 1\n",
    "        print(ngrams)  # Output each bigram to review its presence in the text data\n",
    "\n",
    "# Add identified bigrams to each document in `texts_for_processing`.\n",
    "# This step includes the bigrams in further analysis or model training as part of the document content.\n",
    "for idx in range(len(texts_for_processing)):\n",
    "    for token in bigram[texts_for_processing[idx]]:\n",
    "        if '_' in token:  # Check if token is a bigram\n",
    "            texts_for_processing[idx].append(token)  # Append bigram to the current document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Important:** Verify file output location and filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for saving processed documents, now including bigrams, in JSON format.\n",
    "filename3 = \"/SpectraSync/data/processed-docs/data-w-bigrams.json\"\n",
    "\n",
    "# Open the specified file in write mode with UTF-8 encoding.\n",
    "with open(filename3, 'w', encoding='utf-8') as jsonfile:\n",
    "    # Save the updated text data with bigrams to the JSON file.\n",
    "    json.dump(texts_for_processing, jsonfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Important:** Verify file output location and filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed text with bigrams in JSON Lines (JSONL) format for efficient line-by-line reading.\n",
    "# JSONL format allows each document to be stored on a separate line, making it ideal for large datasets.\n",
    "write_tokenized_sentences_to_jsonl(\n",
    "    texts_for_processing, \n",
    "    \"/SpectraSync/data/processed-docs/data-w-bigrams.jsonl\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
